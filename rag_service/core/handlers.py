"""
í†µí•© ì§€ì‹ ì²˜ë¦¬ê¸° (ë¶ˆë³€ & ê°€ë³€)

Base í´ë˜ìŠ¤: KnowledgeHandler
Subclass: ImmutableKnowledgeHandler, MutableKnowledgeHandler

âœ¨ ì£¼ìš” ê¸°ëŠ¥:
1. ë¶ˆë³€ ì§€ì‹ (Gemini File Search) + ê°€ë³€ ì§€ì‹ (OpenAI) í†µí•© ì²˜ë¦¬
2. ê°„ì†Œí™”ëœ ì¿¼ë¦¬ ì „ëµ
"""

import logging
import importlib
from typing import Dict, Literal, List
from abc import ABC, abstractmethod
import openai

from .config import (
    GEMINI_API_KEY,
    GEMINI_MODEL,
    OPENAI_API_KEY,
    OPENAI_MUTABLE_MODEL,
    DEFAULT_TEMPERATURE,
    DEFAULT_MAX_TOKENS,
    MUTABLE_DEFAULT_TEMPERATURE,
    MUTABLE_DEFAULT_MAX_TOKENS,
    USE_CONTEXT_CACHING
)
from .file_manager import get_file_manager, get_mutable_file_manager

logger = logging.getLogger(__name__)


# ============================================================
# Base í´ë˜ìŠ¤: KnowledgeHandler
# ============================================================

class KnowledgeHandler(ABC):
    """
    ì§€ì‹ ì²˜ë¦¬ ê¸°ë³¸ í´ë˜ìŠ¤
    
    ê³µí†µ ê¸°ëŠ¥:
    - íŒŒì¼ ê´€ë¦¬ì ì´ˆê¸°í™”
    - RAG ì¿¼ë¦¬ ì²˜ë¦¬
    - ì•ˆì „ í•„í„° ìš°íšŒ (ì˜ë¬¸ë¬¸ ë³€í™˜)
    - ì‘ë‹µ ë©”íƒ€ë°ì´í„° ìƒì„±
    """
    
    def __init__(self, knowledge_type: Literal["immutable", "mutable"]):
        """
        Args:
            knowledge_type: "immutable" (ë¶ˆë³€ ì§€ì‹) ë˜ëŠ” "mutable" (ê°€ë³€ ì§€ì‹)
        """
        self.knowledge_type = knowledge_type
        self.model_name = GEMINI_MODEL
        self.uploaded_files = []
        
        # íŒŒì¼ ê´€ë¦¬ì ì´ˆê¸°í™”
        if knowledge_type == "immutable":
            self.file_manager = get_file_manager()
            self._init_immutable()
        else:
            self.file_manager = get_mutable_file_manager()
            self._init_mutable()
        
        logger.info(f"ğŸ¤– {self._get_emoji()} {knowledge_type.upper()} ì²˜ë¦¬ê¸° ì´ˆê¸°í™” ì™„ë£Œ (ìºì‹±: {USE_CONTEXT_CACHING})")
    
    def _get_emoji(self) -> str:
        """ì§€ì‹ íƒ€ì…ë³„ ì´ëª¨í‹°ì½˜"""
        return "ğŸ“š" if self.knowledge_type == "immutable" else "ğŸ“°"
    
    def _get_labels(self) -> Dict[str, str]:
        """ì§€ì‹ íƒ€ì…ë³„ ë¼ë²¨ ë° ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸"""
        if self.knowledge_type == "immutable":
            return {
                "init_msg": "ë¶ˆë³€ ì§€ì‹ íŒŒì¼ ì´ˆê¸°í™” ì¤‘...",
                "query_msg": "ë¶ˆë³€ ì§€ì‹ ì¿¼ë¦¬: ",
                "complete_msg": "ë¶ˆë³€ ì§€ì‹ ë‹µë³€ ì™„ë£Œ",
                "error_msg": "ë¶ˆë³€ ì§€ì‹ ì¿¼ë¦¬ ì‹¤íŒ¨",
                "system_instruction": (
                    "ë‹¹ì‹ ì€ í¼ìŠ¤ë„ ì»¬ëŸ¬ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. "
                    "ì œê³µëœ í¼ìŠ¤ë„ ì»¬ëŸ¬ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ "
                    "ì •í™•í•˜ê³  ìƒì„¸í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”."
                ),
                "source": "immutable_knowledge",
                "no_files_error": "ì‚¬ìš© ê°€ëŠ¥í•œ ë¶ˆë³€ ì§€ì‹ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤."
            }
        else:
            return {
                "init_msg": "ê°€ë³€ ì§€ì‹ íŒŒì¼ ë™ê¸°í™” ì¤‘...",
                "query_msg": "ê°€ë³€ ì§€ì‹ ì¿¼ë¦¬: ",
                "complete_msg": "ê°€ë³€ ì§€ì‹ ë‹µë³€ ì™„ë£Œ",
                "error_msg": "ê°€ë³€ ì§€ì‹ ì¿¼ë¦¬ ì‹¤íŒ¨",
                "system_instruction": (
                    "ë‹¹ì‹ ì€ íŒ¨ì…˜ íŠ¸ë Œë“œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. "
                    "Vogue Koreaì˜ ìµœì‹  íŒ¨ì…˜ íŠ¸ë Œë“œ ê¸°ì‚¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ "
                    "í˜„ì¬ ìœ í–‰í•˜ëŠ” ìŠ¤íƒ€ì¼, ì»¬ëŸ¬, ì•„ì´í…œì— ëŒ€í•œ ì •í™•í•œ ì •ë³´ë¥¼ ì œê³µí•˜ì„¸ìš”."
                ),
                "source": "mutable_knowledge",
                "no_files_error": "ì‚¬ìš© ê°€ëŠ¥í•œ íŠ¸ë Œë“œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
            }
    
    @abstractmethod
    def _init_immutable(self):
        """ë¶ˆë³€ ì§€ì‹ ì´ˆê¸°í™” (subclass êµ¬í˜„ í•„ìš”)"""
        pass
    
    @abstractmethod
    def _init_mutable(self):
        """ê°€ë³€ ì§€ì‹ ì´ˆê¸°í™” (subclass êµ¬í˜„ í•„ìš”)"""
        pass
    
    def _load_files(self, method_name: str = None):
        """
        íŒŒì¼ ë¡œë“œ (ê³µí†µ ë¡œì§)
        
        ë¶ˆë³€ ì§€ì‹: verify_and_repair_files() í˜¸ì¶œ
        ê°€ë³€ ì§€ì‹: sync_files() í˜¸ì¶œ
        """
        labels = self._get_labels()
        logger.info(labels["init_msg"])
        
        if self.knowledge_type == "immutable":
            verified_file_ids = self.file_manager.verify_and_repair_files()
        else:
            verified_file_ids = self.file_manager.sync_files()
        
        if not verified_file_ids:
            if self.knowledge_type == "immutable":
                logger.error("âŒ ì‚¬ìš© ê°€ëŠ¥í•œ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤!")
            else:
                logger.warning("âš ï¸  ì‚¬ìš© ê°€ëŠ¥í•œ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤!")
            return
        
        self.uploaded_files = self.file_manager.get_active_files(verified_file_ids)
        logger.info(f"âœ… {self.knowledge_type} ì§€ì‹ íŒŒì¼ {len(self.uploaded_files)}ê°œ ë¡œë“œ ì™„ë£Œ\n")
    
    # ============================================================
    # í•µì‹¬ ê¸°ëŠ¥: RAG ì¿¼ë¦¬ ì²˜ë¦¬
    # ============================================================
    
    def query(
        self, 
        question: str, 
        temperature: float = None,
        max_tokens: int = None
    ) -> Dict:
        """
        ì§€ì‹ ê¸°ë°˜ ì§ˆë¬¸ ë‹µë³€ (ê³µí†µ ë¡œì§)
        
        ì „ëµ:
        1. ì›ë³¸ ì§ˆë¬¸ìœ¼ë¡œ Gemini í˜¸ì¶œ
        2. ì•ˆì „ í•„í„° ì‹¤íŒ¨ (finish_reason=2) ì‹œ:
           - 1ì°¨ ì •ì œ: OpenAIë¡œ ì˜ë¬¸ë¬¸ ë³€í™˜ â†’ Gemini ì¬í˜¸ì¶œ
           - 2ì°¨ ì •ì œ: ê·¹ë‹¨ì  ê°„ì†Œí™” ("~ì€?") â†’ Gemini ìµœì¢… ì‹œë„
        
        Args:
            question: ì‚¬ìš©ì ì§ˆë¬¸
            temperature: ìƒì„± ì˜¨ë„ (Noneì´ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©)
            max_tokens: ìµœëŒ€ í† í° ìˆ˜ (Noneì´ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©)
            
        Returns:
            {
                "success": bool,
                "answer": str,
                "metadata": dict
            }
        """
        # ê¸°ë³¸ê°’ ì„¤ì •
        if temperature is None:
            temperature = (
                DEFAULT_TEMPERATURE 
                if self.knowledge_type == "immutable" 
                else MUTABLE_DEFAULT_TEMPERATURE
            )
        if max_tokens is None:
            max_tokens = (
                DEFAULT_MAX_TOKENS 
                if self.knowledge_type == "immutable" 
                else MUTABLE_DEFAULT_MAX_TOKENS
            )
        
        try:
            # íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            if not self.uploaded_files:
                labels = self._get_labels()
                raise Exception(labels["no_files_error"])
            
            labels = self._get_labels()
            logger.info(f"{self._get_emoji()} {labels['query_msg']}{question[:50]}...")
            
            # ë¶ˆë³€ ì§€ì‹: File Search ìŠ¤í† ì–´ ì‚¬ìš© (Gemini + google.genai Client)
            store_name = getattr(self, 'file_search_store_name', None)
            if store_name:
                logger.info(f"ğŸ“‚ File Search ìŠ¤í† ì–´ ì‚¬ìš©: {store_name}")
                try:
                    response = self.file_manager.query_file_search_store(
                        store_name=store_name,
                        prompt=question,
                        model=self.model_name
                    )
                    
                    # âœ… None ì‘ë‹µ ëª…ì‹œì  ì²˜ë¦¬
                    if response is None:
                        logger.error(f"âŒ File Search ì¿¼ë¦¬ ì‘ë‹µì´ Noneì…ë‹ˆë‹¤")
                        return None
                    
                    # âœ… ì‘ë‹µ ê²€ì¦
                    if hasattr(response, 'text') and response.text:
                        logger.info(f"âœ… File Search ì‘ë‹µ ì„±ê³µ")
                        answer = response.text
                        
                        # ì¸ìš© ì •ë³´ ì¶”ì¶œ (grounding_metadata)
                        citations = None
                        if hasattr(response, 'candidates') and response.candidates:
                            candidate = response.candidates[0]
                            if hasattr(candidate, 'grounding_metadata'):
                                citations = candidate.grounding_metadata
                        
                        return {
                            "success": True,
                            "answer": answer,
                            "metadata": {
                                "source": "file_search",
                                "route": 2,
                                "model": self.model_name,
                                "citations": citations,
                                "files_used": len(self.uploaded_files),  # âœ… ì¶”ê°€: í†µí•© ì²˜ë¦¬ì—ì„œ ë©”íƒ€ë°ì´í„° ì¼ê´€ì„±
                                "retrieval_method": "gemini_file_search"
                            }
                        }
                    else:
                        logger.error(f"âŒ File Search ì‘ë‹µì— í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤")
                        return None
                        
                except Exception as e:
                    logger.error(f"âŒ File Search ì¿¼ë¦¬ ì‹¤íŒ¨: {e}", exc_info=True)
                    return None
            else:
                logger.error(f"âŒ File Search ìŠ¤í† ì–´ ì´ë¦„ì´ ì—†ìŠµë‹ˆë‹¤")
                return None
            
        except Exception as e:
            labels = self._get_labels()
            logger.error(f"âŒ {labels['error_msg']}: {e}", exc_info=True)
            raise e
    
    # ============================================================
    # í—¬í¼ ë©”ì„œë“œë“¤
    # ============================================================
    
    def _prepare_content_parts(self, question: str) -> List:
        """
        ì½˜í…ì¸  íŒŒíŠ¸ ì¤€ë¹„ (ê°€ë³€ ì§€ì‹ìš©ë§Œ - ë¶ˆë³€ ì§€ì‹ì€ File Search ì‚¬ìš©)
        
        ê°€ë³€ ì§€ì‹(OpenAI): ìµœëŒ€ 5ê°œ ë¬¸ì„œ, 20,000ì ì œí•œ
        ë¶ˆë³€ ì§€ì‹(Gemini): File Search ì‚¬ìš©í•˜ë¯€ë¡œ ì—¬ê¸°ì„œ ì²˜ë¦¬ ì•ˆ í•¨
        """
        if self.knowledge_type == "mutable":
            # ê°€ë³€ ì§€ì‹: ë¬¸ì„œ ê°œìˆ˜/ê¸¸ì´ ì œí•œ ì ìš©
            MAX_DOCS = 5
            MAX_TOTAL_CHARS = 20000
            
            docs = list(self.uploaded_files[-MAX_DOCS:])
            
            # ë¬¸ìì—´ íƒ€ì…ë§Œ ê¸¸ì´ ê³„ì‚°
            string_total_chars = sum(len(d) for d in docs if isinstance(d, str))
            
            if string_total_chars > MAX_TOTAL_CHARS:
                ratio = MAX_TOTAL_CHARS / string_total_chars
                truncated = []
                for d in docs:
                    if isinstance(d, str):
                        keep = max(200, int(len(d) * ratio))
                        truncated.append(d[:keep])
                    else:
                        truncated.append(d)
                docs = truncated
            
            return docs + [question]
        else:
            # ë¶ˆë³€ ì§€ì‹: File Search ì‚¬ìš©í•˜ë¯€ë¡œ ì§ˆë¬¸ë§Œ ë°˜í™˜
            return [question]
    
    def _call_gemini_with_retry(self, model, content_parts, max_retries: int = 3):
        """
        Gemini API í˜¸ì¶œ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)
        
        Args:
            model: Gemini ëª¨ë¸ ê°ì²´
            content_parts: ì½˜í…ì¸  ë¦¬ìŠ¤íŠ¸
            max_retries: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜
            
        Returns:
            Gemini response ê°ì²´
        """
        import time
        
        for attempt in range(1, max_retries + 1):
            try:
                response = model.generate_content(content_parts)
                return response
            except Exception as exc:
                msg = str(exc)
                if attempt < max_retries:
                    logger.warning(f"âš ï¸ ëª¨ë¸ í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ {attempt}/{max_retries}): {msg} - ì¬ì‹œë„ ì¤‘")
                    time.sleep(0.5 * (2 ** (attempt - 1)))
                    continue
                else:
                    logger.error(f"âŒ ì¬ì‹œë„ ì‹¤íŒ¨: {exc}")
                    raise


# ============================================================
# Subclass 1: ImmutableKnowledgeHandler (ë¶ˆë³€ ì§€ì‹)
# ============================================================

class ImmutableKnowledgeHandler(KnowledgeHandler):
    """
    ë¶ˆë³€ ì§€ì‹ ì²˜ë¦¬ê¸° (í¼ìŠ¤ë„ ì»¬ëŸ¬)
    
    íŠ¹ì§•:
    - Gemini API + File Search ì‚¬ìš©
    - PDF íŒŒì¼ (ì„œë²„ ì¸¡ íŒŒì¼ ê²€ìƒ‰)
    """
    
    def __init__(self):
        super().__init__(knowledge_type="immutable")
        # attempt to ensure immutable knowledge is indexed in File Search
        try:
            store_name = self.file_manager.import_all_immutable_to_file_search()
            if store_name:
                self.file_search_store_name = store_name
                logger.info(f"ğŸ“‚ Immutable FileSearch store ì¤€ë¹„ë¨: {store_name}")
            else:
                self.file_search_store_name = None
        except Exception:
            self.file_search_store_name = None
    
    def _init_immutable(self):
        """ë¶ˆë³€ ì§€ì‹ ì´ˆê¸°í™”"""
        # legacy: load files into memory for direct Gemini use
        self._load_files()
    
    def _init_mutable(self):
        """ë¶ˆë³€ ì§€ì‹ì—ì„œëŠ” ë¯¸ì‚¬ìš©"""
        pass



# ============================================================
# Subclass 2: MutableKnowledgeHandler (ê°€ë³€ ì§€ì‹)
# ============================================================

class MutableKnowledgeHandler:
    """
    ê°€ë³€ ì§€ì‹ ì²˜ë¦¬ê¸° (Vogue íŠ¸ë Œë“œ)
    
    íŠ¹ì§•:
    - OpenAI API ì‚¬ìš© (GPT-4o-mini)
    - ë¡œì»¬ í…ìŠ¤íŠ¸ íŒŒì¼ (ìµœëŒ€ 5ê°œ)
    - ì•ˆì „ í•„í„° ìš°íšŒ ë¶ˆí•„ìš” (OpenAIëŠ” ë” ê´€ëŒ€í•¨)
    """
    
    def __init__(self):
        """OpenAI ê¸°ë°˜ ê°€ë³€ ì§€ì‹ ì²˜ë¦¬ê¸° ì´ˆê¸°í™”"""
        self.knowledge_type = "mutable"
        self.file_manager = get_mutable_file_manager()
        self.uploaded_files = []
        self.openai_client = openai.OpenAI(api_key=OPENAI_API_KEY)
        self.model_name = OPENAI_MUTABLE_MODEL
        
        logger.info(f"ğŸ¤– ğŸ“° OpenAI ê¸°ë°˜ MUTABLE ì²˜ë¦¬ê¸° ì´ˆê¸°í™” ì¤‘...")
        self._load_files()
        logger.info(f"ğŸ¤– ğŸ“° OpenAI ê¸°ë°˜ MUTABLE ì²˜ë¦¬ê¸° ì´ˆê¸°í™” ì™„ë£Œ (ëª¨ë¸: {self.model_name})")
    
    def _load_files(self):
        """ê°€ë³€ ì§€ì‹ íŒŒì¼ ë¡œë“œ"""
        logger.info("ê°€ë³€ ì§€ì‹ íŒŒì¼ ë™ê¸°í™” ì¤‘...")
        
        verified_file_ids = self.file_manager.sync_files()
        
        if not verified_file_ids:
            logger.warning("âš ï¸  ì‚¬ìš© ê°€ëŠ¥í•œ ê°€ë³€ ì§€ì‹ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤!")
            return
        
        # ê°€ë³€ ì§€ì‹ì€ ë¡œì»¬ í…ìŠ¤íŠ¸ë¡œ ë¡œë“œ (OpenAI ì „ì†¡ìš©)
        self.uploaded_files = self.file_manager.get_active_files(verified_file_ids)
        logger.info(f"âœ… ê°€ë³€ ì§€ì‹ íŒŒì¼ {len(self.uploaded_files)}ê°œ ë¡œë“œ ì™„ë£Œ\n")
    
    def query(
        self,
        question: str,
        temperature: float = None,
        max_tokens: int = None
    ) -> Dict:
        """
        OpenAI API ê¸°ë°˜ ê°€ë³€ ì§€ì‹ ì¿¼ë¦¬
        
        Args:
            question: ì‚¬ìš©ì ì§ˆë¬¸
            temperature: ìƒì„± ì˜¨ë„
            max_tokens: ìµœëŒ€ í† í° ìˆ˜
            
        Returns:
            {
                "success": bool,
                "answer": str,
                "metadata": dict
            }
        """
        # ê¸°ë³¸ê°’ ì„¤ì •
        if temperature is None:
            temperature = MUTABLE_DEFAULT_TEMPERATURE
        if max_tokens is None:
            max_tokens = MUTABLE_DEFAULT_MAX_TOKENS
        
        try:
            if not self.uploaded_files:
                raise Exception("ì‚¬ìš© ê°€ëŠ¥í•œ ê°€ë³€ ì§€ì‹ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            
            logger.info(f"ğŸ“° ê°€ë³€ ì§€ì‹ ì¿¼ë¦¬ (OpenAI): {question[:50]}...")
            
            # ê°€ë³€ ì§€ì‹ ë¬¸ì„œ ì¤€ë¹„ (ìµœëŒ€ 5ê°œ, 30,000ì)
            MAX_DOCS = 5
            MAX_TOTAL_CHARS = 30000
            
            # ìµœì‹  ë¬¸ì„œ ìš°ì„  (ë¦¬ìŠ¤íŠ¸ ëì´ ìµœì‹ ì´ë¼ê³  ê°€ì •)
            docs = []
            total_chars = 0
            
            for doc in reversed(self.uploaded_files):
                if isinstance(doc, str):
                    if len(docs) >= MAX_DOCS:
                        break
                    if total_chars + len(doc) > MAX_TOTAL_CHARS:
                        # í˜„ì¬ ë¬¸ì„œë¥¼ ë¶€ë¶„ì ìœ¼ë¡œ ì¶”ê°€
                        remaining = MAX_TOTAL_CHARS - total_chars
                        if remaining > 500:
                            docs.append(doc[:remaining])
                        break
                    docs.append(doc)
                    total_chars += len(doc)
            
            # ë¬¸ì„œ ì—­ìˆœ ì •ë ¬ (ìµœì‹ ìˆœ ìœ ì§€)
            docs.reverse()
            
            # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì¤€ë¹„
            doc_text = ""
            for i, doc in enumerate(docs):
                if len(doc) > 1000:
                    doc_text += f"### ìë£Œ {i+1}\n{doc[:1000]}...\n\n"
                else:
                    doc_text += f"### ìë£Œ {i+1}\n{doc}\n\n"
            
            system_prompt = f"""ë‹¹ì‹ ì€ íŒ¨ì…˜ íŠ¸ë Œë“œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì œê³µëœ Vogue Korea íŠ¸ë Œë“œ ìë£Œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•˜ê³  ìƒì„¸í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.

ì œê³µëœ ìë£Œ:
{doc_text}"""
            
            # OpenAI API í˜¸ì¶œ (ì¬ì‹œë„ í¬í•¨)
            max_retries = 3
            response = None
            for attempt in range(1, max_retries + 1):
                try:
                    response = self.openai_client.chat.completions.create(
                        model=self.model_name,
                        messages=[
                            {"role": "system", "content": system_prompt},
                            {"role": "user", "content": question}
                        ],
                        temperature=temperature,
                        max_tokens=max_tokens
                    )
                    break
                except Exception as exc:
                    msg = str(exc)
                    if attempt < max_retries:
                        logger.warning(f"âš ï¸  OpenAI í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ {attempt}): {msg} - ì¬ì‹œë„ ì¤‘")
                        import time
                        time.sleep(0.5 * (2 ** (attempt - 1)))
                        continue
                    else:
                        logger.error(f"âŒ ì¬ì‹œë„ ì‹¤íŒ¨: {exc}")
                        raise
            
            answer = response.choices[0].message.content
            
            metadata = {
                "source": "mutable_knowledge",
                "model": self.model_name,
                "api": "openai",
                "files_used": len(docs),
                "total_chars": total_chars
            }
            
            logger.info(f"ğŸ“° ê°€ë³€ ì§€ì‹ ë‹µë³€ ì™„ë£Œ (OpenAI)\n")
            
            return {
                "success": True,
                "answer": answer,
                "metadata": metadata
            }
            
        except Exception as e:
            logger.error(f"âŒ ê°€ë³€ ì§€ì‹ ì¿¼ë¦¬ ì‹¤íŒ¨: {e}")
            raise e
    
    def resync(self):
        """íŒŒì¼ ì¬ë™ê¸°í™”"""
        self._load_files()


# ============================================================
# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
# ============================================================

_immutable_handler = None
_mutable_handler = None


def get_immutable_handler() -> ImmutableKnowledgeHandler:
    """ë¶ˆë³€ ì§€ì‹ ì²˜ë¦¬ê¸° ì‹±ê¸€í†¤"""
    global _immutable_handler
    if _immutable_handler is None:
        _immutable_handler = ImmutableKnowledgeHandler()
    return _immutable_handler


def get_mutable_handler() -> MutableKnowledgeHandler:
    """ê°€ë³€ ì§€ì‹ ì²˜ë¦¬ê¸° ì‹±ê¸€í†¤"""
    global _mutable_handler
    if _mutable_handler is None:
        _mutable_handler = MutableKnowledgeHandler()
    return _mutable_handler
