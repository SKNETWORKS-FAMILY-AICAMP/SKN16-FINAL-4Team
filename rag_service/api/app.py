"""
í†µí•© ì§€ì‹ RAG API ì„œë²„

ê¸°ëŠ¥:
1. OpenAI ê¸°ë°˜ ì§€ëŠ¥í˜• ë¼ìš°íŒ… (GPT-4o-mini)
2. ë¶ˆë³€ ì§€ì‹ (í¼ìŠ¤ë„ ì»¬ëŸ¬) + ê°€ë³€ ì§€ì‹ (Vogue íŠ¸ë Œë“œ) í†µí•©
3. Context Caching ì˜µì…˜í™” (ê°œë°œ: OFF, í”„ë¡œë•ì…˜: ON)
4. ë‹¨ì¼ ì—”ë“œí¬ì¸íŠ¸ë¡œ ëª¨ë“  ì§€ì‹ ì ‘ê·¼

ë¼ìš°íŒ…:
1. RAG ë¶ˆí•„ìš” (ì¼ë°˜ ëŒ€í™”)
2. ë¶ˆë³€ ì§€ì‹ë§Œ
3. ê°€ë³€ ì§€ì‹ë§Œ
4. ë¶ˆë³€ + ê°€ë³€ í†µí•©
"""

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
from typing import Optional, Dict
from datetime import datetime
import logging
# google.generativeaiëŠ” core ëª¨ë“ˆì—ì„œ ì‚¬ìš©ë˜ë¯€ë¡œ ì´ê³³ì—ì„œëŠ” ë¶ˆí•„ìš”í•˜ì—¬ ì œê±°

# ============================================================
# ì„¤ì • ë° í•¸ë“¤ëŸ¬ import
# ============================================================

from ..core import (
    USE_CONTEXT_CACHING,
    DEFAULT_TEMPERATURE,
    DEFAULT_MAX_TOKENS,
    get_router,
    get_immutable_handler,
    get_mutable_handler,
)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ============================================================
# FastAPI ì•± ì„¤ì •
# ============================================================

app = FastAPI(
    title="í†µí•© ì§€ì‹ RAG API",
    description="í¼ìŠ¤ë„ ì»¬ëŸ¬ + íŒ¨ì…˜ íŠ¸ë Œë“œ í†µí•© ì§€ì‹ ì‹œìŠ¤í…œ",
    version="2.0.0"
)

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ============================================================
# Pydantic ëª¨ë¸ ì •ì˜
# ============================================================

class UnifiedQueryRequest(BaseModel):
    """í†µí•© ì§€ì‹ ê²€ìƒ‰ ìš”ì²­"""
    query: str = Field(..., description="ì‚¬ìš©ì ì§ˆë¬¸")
    temperature: Optional[float] = Field(DEFAULT_TEMPERATURE, description="ìƒì„± ì˜¨ë„")
    max_tokens: Optional[int] = Field(DEFAULT_MAX_TOKENS, description="ìµœëŒ€ í† í° ìˆ˜")
    force_route: Optional[int] = Field(None, description="ê°•ì œ ë¼ìš°íŒ… (1-4, í…ŒìŠ¤íŠ¸ìš©)")
    
    class Config:
        json_schema_extra = {
            "example": {
                "query": "ë´„ ì›œí†¤ì—ê²Œ ì–´ìš¸ë¦¬ëŠ” 2025ë…„ íŠ¸ë Œë“œ ë¦½ìŠ¤í‹± ì¶”ì²œí•´ì¤˜",
                "temperature": 0.7,
                "max_tokens": 2048
            }
        }


class UnifiedQueryResponse(BaseModel):
    """í†µí•© ì§€ì‹ ê²€ìƒ‰ ì‘ë‹µ"""
    success: bool
    answer: str
    query: str
    route: int = Field(..., description="ë¼ìš°íŒ… ê²°ê³¼ (1-4)")
    route_description: str
    sources: list[str] = Field(default_factory=list)
    metadata: Dict
    timestamp: str


class HealthCheckResponse(BaseModel):
    """í—¬ìŠ¤ ì²´í¬ ì‘ë‹µ"""
    status: str
    immutable_files: int
    mutable_files: int
    caching_enabled: bool
    router_model: str
    timestamp: str


# ============================================================
# í†µí•© ì§€ì‹ RAG ì‹œìŠ¤í…œ
# ============================================================

class UnifiedKnowledgeRAG:
    """
    í†µí•© ì§€ì‹ RAG ì‹œìŠ¤í…œ
    
    ë¼ìš°íŒ… â†’ ì§€ì‹ ì²˜ë¦¬ â†’ ì‘ë‹µ ìƒì„±
    """
    
    def __init__(self):
        # ê° ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.router = get_router()
        self.immutable_handler = get_immutable_handler()
        self.mutable_handler = get_mutable_handler()
        
        logger.info("="*60)
        logger.info("ğŸš€ í†µí•© ì§€ì‹ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info(f"   Context Caching: {'ON' if USE_CONTEXT_CACHING else 'OFF'}")
        logger.info(f"   ë¶ˆë³€ ì§€ì‹: {len(self.immutable_handler.uploaded_files)}ê°œ íŒŒì¼")
        logger.info(f"   ê°€ë³€ ì§€ì‹: {len(self.mutable_handler.uploaded_files)}ê°œ íŒŒì¼")
        logger.info("="*60 + "\n")
    
    def query(
        self,
        question: str,
        temperature: float = DEFAULT_TEMPERATURE,
        max_tokens: int = DEFAULT_MAX_TOKENS,
        force_route: Optional[int] = None
    ) -> Dict:
        """
        í†µí•© ì§ˆë¬¸ ì²˜ë¦¬
        
        íë¦„:
        1. ë¼ìš°íŒ… íŒë‹¨ (OpenAI)
        2. ì§€ì‹ ì†ŒìŠ¤ ì„ íƒ
        3. RAG ì‹¤í–‰
        4. ì‘ë‹µ ë°˜í™˜
        """
        try:
            logger.info("="*60)
            logger.info(f"ğŸ“¥ ì§ˆë¬¸: {question}")
            logger.info("="*60)
            
            # 1. ë¼ìš°íŒ… (ê°•ì œ ë¼ìš°íŒ… ë˜ëŠ” ìë™ íŒë‹¨)
            if force_route:
                route = force_route
                logger.info(f"âš¡ ê°•ì œ ë¼ìš°íŒ…: {route}")
            else:
                route = self.router.route(question)
            
            route_desc = self.router.get_route_description(route)
            
            # 2. ë¼ìš°íŒ…ì— ë”°ë¼ ì²˜ë¦¬
            if route == 1:
                # RAG ë¶ˆí•„ìš” - ê¸°ë³¸ ì‘ë‹µ
                answer = self._handle_general(question)
                sources = []
                metadata = {
                    "route": route,
                    "route_description": route_desc,
                    "rag_used": False
                }
            
            elif route == 2:
                # ë¶ˆë³€ ì§€ì‹ë§Œ (ì›ë³¸ query() ì‚¬ìš©, ì•ˆì „ í•„í„° ìš°íšŒ ë¡œì§ í¬í•¨)
                result = self.immutable_handler.query(question, temperature, max_tokens)
                
                # âœ… None ì‘ë‹µ ì²˜ë¦¬
                if result is None:
                    logger.error(f"âŒ ë¶ˆë³€ ì§€ì‹ í•¸ë“¤ëŸ¬ ì¿¼ë¦¬ ì‹¤íŒ¨ (None ì‘ë‹µ)")
                    raise RuntimeError("ë¶ˆë³€ ì§€ì‹ ì¿¼ë¦¬ ì‹¤íŒ¨: ìœ íš¨í•œ ì‘ë‹µ ì—†ìŒ")
                
                answer = result['answer']
                sources = ["immutable_knowledge"]
                metadata = {
                    "route": route,
                    "route_description": route_desc,
                    "rag_used": True,
                    **result['metadata']
                }
            
            elif route == 3:
                # ê°€ë³€ ì§€ì‹ë§Œ (ì‹¤íŒ¨ ì‹œ ë¶ˆë³€ ì§€ì‹ìœ¼ë¡œ í´ë°±)
                try:
                    result = self.mutable_handler.query(question, temperature, max_tokens)
                    answer = result['answer']
                    sources = ["mutable_knowledge"]
                    metadata = {
                        "route": route,
                        "route_description": route_desc,
                        "rag_used": True,
                        **result['metadata']
                    }
                except Exception as e:
                    logger.warning(f"âš ï¸  ê°€ë³€ ì§€ì‹ ì¿¼ë¦¬ ì‹¤íŒ¨: {e}. ë¶ˆë³€ ì§€ì‹ìœ¼ë¡œ í´ë°±í•©ë‹ˆë‹¤.")
                    # ë¶ˆë³€ ì§€ì‹ìœ¼ë¡œ í´ë°±
                    result = self.immutable_handler.query(question, temperature, max_tokens)
                    answer = result['answer']
                    sources = ["immutable_knowledge (fallback)"]
                    metadata = {
                        "route": 2,  # ì‹¤ì œë¡œëŠ” 2ë²ˆ ê²½ë¡œ ì‚¬ìš©
                        "route_description": "Fallback to immutable knowledge",
                        "rag_used": True,
                        "fallback_from_route": route,
                        **result['metadata']
                    }
            
            elif route == 4:
                # ë¶ˆë³€ + ê°€ë³€ í†µí•© (ì‹¤íŒ¨ ì‹œ ë¶ˆë³€ë§Œ ì‚¬ìš©)
                try:
                    answer, sources, metadata = self._handle_combined(
                        question, temperature, max_tokens
                    )
                    metadata["route"] = route
                    metadata["route_description"] = route_desc
                except Exception as e:
                    logger.warning(f"âš ï¸  í†µí•© ì¿¼ë¦¬ ì‹¤íŒ¨: {e}. ë¶ˆë³€ ì§€ì‹ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                    # ë¶ˆë³€ ì§€ì‹ë§Œìœ¼ë¡œ í´ë°±
                    result = self.immutable_handler.query(question, temperature, max_tokens)
                    answer = result['answer']
                    sources = ["immutable_knowledge (fallback)"]
                    metadata = {
                        "route": 2,
                        "route_description": "Fallback to immutable knowledge",
                        "rag_used": True,
                        "fallback_from_route": route,
                        **result['metadata']
                    }
            
            else:
                raise ValueError(f"ì˜ëª»ëœ ë¼ìš°íŒ…: {route}")
            
            logger.info(f"âœ… ì²˜ë¦¬ ì™„ë£Œ: {route_desc}\n")
            
            return {
                "success": True,
                "answer": answer,
                "query": question,
                "route": route,
                "route_description": route_desc,
                "sources": sources,
                "metadata": metadata
            }
            
        except Exception as e:
            logger.error(f"âŒ í†µí•© ì¿¼ë¦¬ ì‹¤íŒ¨: {e}")
            raise e
    
    def _handle_general(self, question: str) -> str:
        """
        ì¼ë°˜ ëŒ€í™” ì²˜ë¦¬ (RAG ì—†ìŒ)
        
        ê°„ë‹¨í•œ ì‘ë‹µ ìƒì„±
        """
        logger.info("ğŸ’¬ ì¼ë°˜ ëŒ€í™” ëª¨ë“œ")
        
        # ê°„ë‹¨í•œ ê¸°ë³¸ ì‘ë‹µ
        responses = {
            "ì•ˆë…•": "ì•ˆë…•í•˜ì„¸ìš”! í¼ìŠ¤ë„ ì»¬ëŸ¬ì™€ íŒ¨ì…˜ íŠ¸ë Œë“œì— ëŒ€í•´ ê¶ê¸ˆí•˜ì‹  ê²ƒì´ ìˆìœ¼ì‹ ê°€ìš”?",
            "ë„ì›€": "í¼ìŠ¤ë„ ì»¬ëŸ¬ ì§„ë‹¨, ìƒ‰ìƒ ì¶”ì²œ, ìµœì‹  íŒ¨ì…˜ íŠ¸ë Œë“œ ë“±ì— ëŒ€í•´ ë„ì›€ì„ ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
        }
        
        for keyword, response in responses.items():
            if keyword in question.lower():
                return response
        
        return "ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”? í¼ìŠ¤ë„ ì»¬ëŸ¬ë‚˜ íŒ¨ì…˜ íŠ¸ë Œë“œì— ëŒ€í•´ ì§ˆë¬¸í•´ì£¼ì„¸ìš”."
    
    
    def _handle_combined(
        self,
        question: str,
        temperature: float,
        max_tokens: int
    ) -> tuple[str, list, Dict]:
        """
        ë¶ˆë³€ + ê°€ë³€ ì§€ì‹ í†µí•© ì²˜ë¦¬
        
        ì „ëµ:
        1. ë¶ˆë³€ ì§€ì‹: File Search (í¼ìŠ¤ë„ ì»¬ëŸ¬ ê´€ì )
        2. ê°€ë³€ ì§€ì‹: OpenAI (ìµœì‹  íŠ¸ë Œë“œ ê´€ì )
        3. ë‘ ë‹µë³€ì„ í†µí•©í•˜ì—¬ ìµœì¢… ì‘ë‹µ ìƒì„±
        """
        logger.info("ğŸ”€ ë¶ˆë³€ + ê°€ë³€ ì§€ì‹ í†µí•© ëª¨ë“œ")
        
        try:
            # ë¶ˆë³€ ì§€ì‹ ì¿¼ë¦¬ (File Search)
            logger.info("  ğŸ“š ë¶ˆë³€ ì§€ì‹ ì¡°íšŒ ì¤‘...")
            immutable_result = self.immutable_handler.query(
                question, temperature, max_tokens
            )
            
            # âœ… None ì‘ë‹µ ì²´í¬
            if immutable_result is None:
                logger.error("âŒ ë¶ˆë³€ ì§€ì‹ ì¿¼ë¦¬ ì‹¤íŒ¨ (None ì‘ë‹µ)")
                raise RuntimeError("ë¶ˆë³€ ì§€ì‹ ì¿¼ë¦¬ ì‹¤íŒ¨")
            
            # ê°€ë³€ ì§€ì‹ ì¿¼ë¦¬ (OpenAI)
            logger.info("  ğŸ“° ê°€ë³€ ì§€ì‹ ì¡°íšŒ ì¤‘...")
            mutable_result = self.mutable_handler.query(
                question, temperature, max_tokens
            )
            
            # âœ… None ì‘ë‹µ ì²´í¬
            if mutable_result is None:
                logger.error("âŒ ê°€ë³€ ì§€ì‹ ì¿¼ë¦¬ ì‹¤íŒ¨ (None ì‘ë‹µ)")
                raise RuntimeError("ê°€ë³€ ì§€ì‹ ì¿¼ë¦¬ ì‹¤íŒ¨")
            
            # ë‘ ë‹µë³€ í†µí•©
            combined_answer = f"""**í¼ìŠ¤ë„ ì»¬ëŸ¬ ê´€ì :**
{immutable_result['answer']}

**ìµœì‹  íŠ¸ë Œë“œ ê´€ì :**
{mutable_result['answer']}

---
ìœ„ ë‘ ê°€ì§€ ê´€ì ì„ ì¢…í•©í•˜ì—¬ ë‹µë³€ë“œë ¸ìŠµë‹ˆë‹¤."""
            
            sources = ["immutable_knowledge", "mutable_knowledge"]
            
            # âœ… ë©”íƒ€ë°ì´í„° ì¼ê´€ì„± ì²˜ë¦¬ (files_used í‚¤ ì¡´ì¬ í™•ì¸)
            immutable_files = immutable_result.get('metadata', {}).get('files_used', 1)
            mutable_files = mutable_result.get('metadata', {}).get('files_used', 0)
            
            metadata = {
                "rag_used": True,
                "immutable_files": immutable_files,
                "mutable_files": mutable_files,
                "combined": True,
                "immutable_retrieval": immutable_result.get('metadata', {}).get('retrieval_method', 'gemini_file_search'),
                "mutable_retrieval": mutable_result.get('metadata', {}).get('retrieval_method', 'openai_rag'),
                "immutable_model": immutable_result.get('metadata', {}).get('model', 'gemini-2.5-flash'),
                "mutable_model": mutable_result.get('metadata', {}).get('model', 'gpt-4o-mini')
            }
            
            logger.info(f"âœ… í†µí•© ì²˜ë¦¬ ì„±ê³µ: ë¶ˆë³€({immutable_files}íŒŒì¼) + ê°€ë³€({mutable_files}íŒŒì¼)")
            
            return combined_answer, sources, metadata
            
        except Exception as e:
            logger.error(f"âŒ í†µí•© ì²˜ë¦¬ ì‹¤íŒ¨: {e}", exc_info=True)
            # í´ë°±: ë¶ˆë³€ ì§€ì‹ë§Œ ì‚¬ìš©
            logger.warning("âš ï¸  í´ë°±: ë¶ˆë³€ ì§€ì‹ë§Œ ì‚¬ìš©")
            result = self.immutable_handler.query(question, temperature, max_tokens)
            
            # âœ… None ì‘ë‹µ ì²´í¬
            if result is None:
                raise RuntimeError("í´ë°±ë„ ì‹¤íŒ¨: ë¶ˆë³€ ì§€ì‹ ì¿¼ë¦¬ ì‹¤íŒ¨")
            
            return result['answer'], ["immutable_knowledge"], result['metadata']


# ============================================================
# RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
# ============================================================

rag_system = UnifiedKnowledgeRAG()


# ============================================================
# FastAPI ì—”ë“œí¬ì¸íŠ¸
# ============================================================

@app.get("/")
async def root():
    """ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    return {
        "service": "í†µí•© ì§€ì‹ RAG API",
        "version": "2.0.0",
        "features": [
            "ì§€ëŠ¥í˜• ë¼ìš°íŒ… (OpenAI GPT-4o-mini)",
            "ë¶ˆë³€ ì§€ì‹ (í¼ìŠ¤ë„ ì»¬ëŸ¬)",
            "ê°€ë³€ ì§€ì‹ (Vogue íŠ¸ë Œë“œ)",
            "í†µí•© ê²€ìƒ‰"
        ],
        "caching": USE_CONTEXT_CACHING,
        "endpoints": {
            "health": "GET /health",
            "query": "POST /query",
            "docs": "GET /docs"
        }
    }


@app.get("/health", response_model=HealthCheckResponse)
async def health_check():
    """í—¬ìŠ¤ ì²´í¬"""
    return HealthCheckResponse(
        status="healthy",
        immutable_files=len(rag_system.immutable_handler.uploaded_files),
        mutable_files=len(rag_system.mutable_handler.uploaded_files),
        caching_enabled=USE_CONTEXT_CACHING,
        router_model=rag_system.router.model,
        timestamp=datetime.now().isoformat()
    )


@app.post("/query", response_model=UnifiedQueryResponse)
async def unified_query(request: UnifiedQueryRequest):
    """
    í†µí•© ì§€ì‹ ê²€ìƒ‰
    
    ìë™ ë¼ìš°íŒ…ìœ¼ë¡œ ìµœì ì˜ ì§€ì‹ ì†ŒìŠ¤ ì„ íƒ
    """
    try:
        result = rag_system.query(
            question=request.query,
            temperature=request.temperature,
            max_tokens=request.max_tokens,
            force_route=request.force_route
        )
        
        return UnifiedQueryResponse(
            **result,
            timestamp=datetime.now().isoformat()
        )
        
    except Exception as e:
        logger.error(f"ì¿¼ë¦¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/sync/mutable")
async def sync_mutable_knowledge():
    """ê°€ë³€ ì§€ì‹ ë™ê¸°í™” (ìƒˆ Vogue ê¸°ì‚¬ ì¶”ê°€ ì‹œ)"""
    try:
        rag_system.mutable_handler.resync()
        
        return {
            "success": True,
            "message": "ê°€ë³€ ì§€ì‹ ë™ê¸°í™” ì™„ë£Œ",
            "files": len(rag_system.mutable_handler.uploaded_files)
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/routing/test")
async def test_routing(question: str):
    """
    ë¼ìš°íŒ… í…ŒìŠ¤íŠ¸ (ê°œë°œìš©)
    
    ì§ˆë¬¸ì´ ì–´ë–»ê²Œ ë¼ìš°íŒ…ë˜ëŠ”ì§€ í™•ì¸
    """
    route = rag_system.router.route(question)
    
    return {
        "question": question,
        "route": route,
        "description": rag_system.router.get_route_description(route),
        "routes": {
            "1": "RAG ë¶ˆí•„ìš”",
            "2": "ë¶ˆë³€ ì§€ì‹ (í¼ìŠ¤ë„ ì»¬ëŸ¬)",
            "3": "ê°€ë³€ ì§€ì‹ (íŠ¸ë Œë“œ)",
            "4": "ë¶ˆë³€ + ê°€ë³€"
        }
    }


# ============================================================
# ì„œë²„ ì§ì ‘ ì‹¤í–‰ (ê°œë°œìš©)
# ============================================================

if __name__ == "__main__":
    import uvicorn
    logger.info("="*60)
    logger.info("ğŸš€ í†µí•© ì§€ì‹ RAG API ì„œë²„ ì‹œì‘")
    logger.info("="*60)
    logger.info(f"ğŸ§  ë¼ìš°í„°: {rag_system.router.model}")
    logger.info(f"ğŸ“š ë¶ˆë³€ ì§€ì‹: {len(rag_system.immutable_handler.uploaded_files)}ê°œ íŒŒì¼")
    logger.info(f"ğŸ“° ê°€ë³€ ì§€ì‹: {len(rag_system.mutable_handler.uploaded_files)}ê°œ íŒŒì¼")
    logger.info(f"ğŸ“¦ Context Caching: {'ON' if USE_CONTEXT_CACHING else 'OFF'}")
    logger.info(f"ğŸŒ ì„œë²„: http://localhost:8000")
    logger.info(f"ğŸ“– API ë¬¸ì„œ: http://localhost:8000/docs")
    logger.info("="*60)

    uvicorn.run(app, host="0.0.0.0", port=8000, log_level="info")
