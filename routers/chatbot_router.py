
from fastapi import APIRouter, HTTPException, Depends, Body, Query
from openai import OpenAI
from dotenv import load_dotenv
from sqlalchemy.orm import Session
from sqlalchemy import func
from datetime import datetime, timezone
import models
from routers.user_router import get_current_user
from database import SessionLocal
import os
import json

from schemas import (
    ChatbotRequest,
    ChatbotHistoryResponse,
    ChatItemModel,
    ChatResModel,
    ReportCreate,
    ReportResponse,
)
from routers.feedback_router import generate_ai_feedbacks
from utils.shared import build_rag_index, analyze_conversation_for_color_tone, normalize_personal_color
from utils.emotion_lottie import lottie_filename, to_canonical
import random
import asyncio

# Optional: load influencer personas from the influencer service if available
try:
    import services.api_influencer.main as influencer_service
except Exception:
    influencer_service = None
try:
    import services.api_color.main as api_color_service
except Exception:
    api_color_service = None
try:
    import services.orchestrator.main as orchestrator_service
except Exception:
    orchestrator_service = None
try:
    import services.api_emotion.main as api_emotion_service
except Exception:
    api_emotion_service = None

load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    raise RuntimeError("í™˜ê²½ë³€ìˆ˜ OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

# ëª¨ë¸ ì„¤ì •
EMOTION_MODEL_ID = os.getenv("EMOTION_MODEL_ID")
DEFAULT_MODEL = os.getenv("DEFAULT_MODEL", "gpt-4.1-nano-2025-04-14")

client = OpenAI(api_key=OPENAI_API_KEY)
router = APIRouter(prefix="/api/chatbot", tags=["Chatbot"])


# ëª¨ë¸ ì„ íƒ í•¨ìˆ˜ (ì¤‘ë³µ ì œê±°)
def get_model_to_use():
    return EMOTION_MODEL_ID if EMOTION_MODEL_ID else DEFAULT_MODEL

# ëª¨ë¸ ìƒíƒœ ì¶œë ¥
print(f"ğŸš€ Chatbot Router ì´ˆê¸°í™”")
print(f"   - ê¸°ë³¸ ëª¨ë¸: {DEFAULT_MODEL}")
if EMOTION_MODEL_ID:
    print(f"   - Fine-tuned ê°ì • ëª¨ë¸: {EMOTION_MODEL_ID[:30]}***")
    print(f"   âœ… Fine-tuned ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥")
else:
    print(f"   âš ï¸ Fine-tuned ëª¨ë¸ ë¯¸ì„¤ì •, ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©")

def generate_complete_diagnosis_data(conversation_text: str, season: str) -> dict:
    """
    OpenAI APIë¥¼ í†µí•´ ì™„ì „í•œ ì§„ë‹¨ ë°ì´í„° ìƒì„±
    """
    try:
        # ëŒ€í™” í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¸¸ë©´ ìš”ì•½
        if len(conversation_text) > 1000:
            conversation_text = conversation_text[:1000] + "...(ìƒëµ)"
        prompt = f"""
    ì‚¬ìš©ìì™€ í¼ìŠ¤ë„ ì»¬ëŸ¬ ì „ë¬¸ê°€ì˜ ëŒ€í™”:
    {conversation_text}

    ìœ„ ëŒ€í™”ë¥¼ ë°”íƒ•ìœ¼ë¡œ {season} íƒ€ì… í¼ìŠ¤ë„ ì»¬ëŸ¬ ì§„ë‹¨ ê²°ê³¼ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.

    ë‹¤ìŒ ìœ íš¨í•œ JSON ê°ì²´ í•˜ë‚˜ë§Œ, ë‹¤ë¥¸ ì„¤ëª… ì—†ì´ ë°˜í™˜í•´ì£¼ì„¸ìš”. JSONì€ ë°˜ë“œì‹œ ì•„ë˜ í‚¤ë“¤ì„ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤:
    {{
        "result_name": "{season} {{primary_or_sub}} í˜•ì‹ì˜ í•œê¸€ ë¬¸ìì—´ ì˜ˆ: 'ê°€ì„ ì›œí†¤'",
        "primary_tone": "'ì›œ' ë˜ëŠ” 'ì¿¨' (ì§§ì€ ë¬¸ìì—´)",
        "sub_tone": "'ë´„','ì—¬ë¦„','ê°€ì„' ë˜ëŠ” 'ê²¨ìš¸' (ì§§ì€ ë¬¸ìì—´)",
        "emotional_description": "ê°ì„±ì ì´ê³  ê¸ì •ì ì¸ í•œ ë¬¸ì¥",
        "color_palette": ["{season} íƒ€ì…ì— ì–´ìš¸ë¦¬ëŠ” 5ê°œì˜ HEX ìƒ‰ìƒ ì½”ë“œ"],
        "style_keywords": ["{season} íƒ€ì…ì˜ íŠ¹ì„±ì„ ë‚˜íƒ€ë‚´ëŠ” 5ê°œ í‚¤ì›Œë“œ"],
        "makeup_tips": ["ì‹¤ìš©ì ì¸ ë©”ì´í¬ì—… íŒ 4ê°œ"],
        "detailed_analysis": "ëŒ€í™” ë‚´ìš©ì„ ë°˜ì˜í•œ ê°œì¸í™”ëœ ë¶„ì„ (2-3ë¬¸ë‹¨, êµ¬ì²´ì ì´ê³  ì‹¤ìš©ì ì¸ ì¡°ì–¸ í¬í•¨)",
        "top_types": [
            {{"name": "{{ê³„ì ˆ}} {{ì›œ/ì¿¨}}í†¤", "type": "spring|summer|autumn|winter", "description": "ê°„ë‹¨ ì„¤ëª…", "score": 0}}
        ]
    }}

    ì¤‘ìš” ìš”êµ¬ì‚¬í•­:
    - `result_name`ê³¼ `top_types` ë°°ì—´ì˜ ê° í•­ëª© `name`ì€ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ "{{ê³„ì ˆ}} {{ì›œ/ì¿¨}}í†¤" í˜•ì‹(ì˜ˆ: "ê°€ì„ ì›œí†¤", "ê²¨ìš¸ ì¿¨í†¤")ì´ì–´ì•¼ í•©ë‹ˆë‹¤.
    - `top_types[0].name`ì€ `result_name`ê³¼ ë™ì¼í•œ ê°’ì´ì–´ì•¼ í•©ë‹ˆë‹¤.
    - `primary_tone`ì€ ë°˜ë“œì‹œ 'ì›œ' ë˜ëŠ” 'ì¿¨'ë¡œ í‘œê¸°í•˜ê³ , `sub_tone`ì€ 'ë´„/ì—¬ë¦„/ê°€ì„/ê²¨ìš¸' ì¤‘ í•˜ë‚˜ë¡œ í‘œê¸°í•˜ì„¸ìš”.
    - ìˆ«ì ê°’(score)ì€ 0~100 ì‚¬ì´ì˜ ì •ìˆ˜ë¡œ í‘œê¸°í•˜ì„¸ìš”.
    - ì¶œë ¥ì€ ì˜¤ì§ í•˜ë‚˜ì˜ JSON ê°ì²´ì—¬ì•¼ í•˜ë©°, ì¶”ê°€ ì„¤ëª… í…ìŠ¤íŠ¸ëŠ” í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.

    ì£¼ì˜ì‚¬í•­:
    - detailed_analysisëŠ” ë°˜ë³µì ì¸ ë‚´ìš© ì—†ì´ ê°œì¸í™”ëœ ë¶„ì„ìœ¼ë¡œ ì‘ì„±
    - ëŒ€í™”ì—ì„œ ì–¸ê¸‰ëœ ê°œì¸ì  íŠ¹ì„±ì„ ë°˜ì˜
    - ì‹¤ìš©ì ì´ê³  êµ¬ì²´ì ì¸ ì¡°ì–¸ í¬í•¨
    - í•œêµ­ì–´ë¡œ ì‘ì„±
    """
        # ëª¨ë¸ ì„ íƒ í•¨ìˆ˜ ì‚¬ìš©
        response = client.chat.completions.create(
            model=get_model_to_use(),
            messages=[{
                "role": "system",
                "content": "ë‹¹ì‹ ì€ í¼ìŠ¤ë„ ì»¬ëŸ¬ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ëŒ€í™”ë¥¼ ë¶„ì„í•˜ì—¬ ì •í™•í•˜ê³  ê°œì¸í™”ëœ ì§„ë‹¨ ê²°ê³¼ë¥¼ ì œê³µí•©ë‹ˆë‹¤."
            }, {
                "role": "user", 
                "content": prompt
            }],
            max_tokens=1000,
            temperature=0.3
        )
        ai_response = response.choices[0].message.content.strip()
        try:
            import re
            json_match = re.search(r'\{.*\}', ai_response, re.DOTALL)
            if json_match:
                json_str = json_match.group()
                result = json.loads(json_str)
                if not result.get("detailed_analysis") or len(result.get("detailed_analysis", "")) < 50:
                    print("âš ï¸ AI ë¶„ì„ ê²°ê³¼ê°€ ë„ˆë¬´ ì§§ìŒ, ê¸°ë³¸ê°’ ì‚¬ìš©")
                    return get_default_diagnosis_data(season)
                return result
        except Exception as parse_error:
            print(f"âŒ AI ì‘ë‹µ JSON íŒŒì‹± ì‹¤íŒ¨: {parse_error}")
            print(f"AI ì‘ë‹µ: {ai_response[:200]}...")
        return get_default_diagnosis_data(season)
    except Exception as e:
        print(f"âŒ OpenAI API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
        return get_default_diagnosis_data(season)

def get_default_diagnosis_data(season: str) -> dict:
    """
    API ì‹¤íŒ¨ ì‹œ ì‚¬ìš©í•  ê¸°ë³¸ ì§„ë‹¨ ë°ì´í„°
    """
    default_data = {
        "ë´„": {
            "emotional_description": "ìƒê¸° ë„˜ì¹˜ê³  í™”ì‚¬í•œ ë‹¹ì‹ ì€ ë´„ ì›œí†¤ íƒ€ì…ì…ë‹ˆë‹¤! ë°ê³  ë”°ëœ»í•œ ìƒ‰ìƒì´ ìì—°ìŠ¤ëŸ½ê²Œ ì–´ìš¸ë¦¬ëŠ” ë§¤ë ¥ì ì¸ ë¶„ì´ì—ìš”.",
            "color_palette": ["#FFB6C1", "#FFA07A", "#FFFF99", "#98FB98", "#87CEEB"],
            "style_keywords": ["ë°ì€", "í™”ì‚¬í•œ", "ìƒë™ê° ìˆëŠ”", "ë”°ëœ»í•œ", "ìì—°ìŠ¤ëŸ¬ìš´"],
            "makeup_tips": ["ì½”ë„ ê³„ì—´ ë¦½ìŠ¤í‹±ìœ¼ë¡œ ìƒê¸° ì—°ì¶œ", "í”¼ì¹˜ ë¸”ëŸ¬ì…”ë¡œ ìì—°ìŠ¤ëŸ¬ìš´ í™ì¡°", "ê³¨ë“œ ì•„ì´ì„€ë„ë¡œ ë”°ëœ»í•œ ëˆˆë§¤", "ë¸Œë¼ìš´ ë§ˆìŠ¤ì¹´ë¼ë¡œ ë¶€ë“œëŸ¬ìš´ ëˆˆë§¤"],
            "detailed_analysis": "ë´„ ì›œí†¤ íƒ€ì…ì¸ ë‹¹ì‹ ì€ ë”°ëœ»í•˜ê³  ë°ì€ ìƒ‰ìƒì´ ê°€ì¥ ì˜ ì–´ìš¸ë¦¬ëŠ” íƒ€ì…ì…ë‹ˆë‹¤.\n\ní‰ì†Œ ë°ê³  ê²½ì¾Œí•œ ì¸ìƒì„ ì£¼ëŠ” ë‹¹ì‹ ì—ê²ŒëŠ” ì½”ë„, í”¼ì¹˜, ì•„ì´ë³´ë¦¬ ê³„ì—´ì˜ ìƒ‰ìƒì´ í”¼ë¶€í†¤ì„ ë”ìš± ìƒë™ê° ìˆê²Œ ë§Œë“¤ì–´ ì¤ë‹ˆë‹¤. ë©”ì´í¬ì—… ì‹œì—ëŠ” ë„ˆë¬´ ì§„í•˜ê±°ë‚˜ ì¿¨í†¤ ê³„ì—´ë³´ë‹¤ëŠ” ìì—°ìŠ¤ëŸ½ê³  ë”°ëœ»í•œ ëŠë‚Œì˜ ìƒ‰ìƒì„ ì„ íƒí•˜ì‹œë©´ ë”ìš± ë§¤ë ¥ì ì¸ ëª¨ìŠµì„ ì—°ì¶œí•  ìˆ˜ ìˆì–´ìš”.\n\níŒ¨ì…˜ì—ì„œë„ í™”ì´íŠ¸, í¬ë¦¼, ì½”ë„, ì—°ë‘ìƒ‰ ë“±ì„ í™œìš©í•˜ì‹œë©´ í™œê¸°ì°¬ ë‹¹ì‹ ì˜ ë§¤ë ¥ì„ í•œì¸µ ë” ë‹ë³´ì´ê²Œ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        },
        "ì—¬ë¦„": {
            "emotional_description": "ì‹œì›í•˜ê³  ìš°ì•„í•œ ë‹¹ì‹ ì€ ì—¬ë¦„ ì¿¨í†¤ íƒ€ì…ì…ë‹ˆë‹¤! ë¶€ë“œëŸ½ê³  ì„¸ë ¨ëœ ìƒ‰ìƒì´ ë‹¹ì‹ ì˜ ìš°ì•„í•¨ì„ ë”ìš± ë¹›ë‚˜ê²Œ í•´ì¤ë‹ˆë‹¤.",
            "color_palette": ["#E6E6FA", "#B0C4DE", "#FFC0CB", "#DDA0DD", "#F0F8FF"],
            "style_keywords": ["ë¶€ë“œëŸ¬ìš´", "ìš°ì•„í•œ", "ì„¸ë ¨ëœ", "ì‹œì›í•œ", "íŒŒìŠ¤í…”"],
            "makeup_tips": ["ë¡œì¦ˆ í•‘í¬ ë¦½ìœ¼ë¡œ ìƒì¾Œí•œ ì¸ìƒ", "ë¼ë²¤ë” ì•„ì´ì„€ë„ë¡œ ëª½í™˜ì  ëˆˆë§¤", "ì‹¤ë²„ í•˜ì´ë¼ì´í„°ë¡œ íˆ¬ëª…í•œ ìœ¤ê¸°", "ì• ì‰¬ ë¸Œë¼ìš´ ì•„ì´ë¸Œë¡œìš°ë¡œ ë¶€ë“œëŸ¬ìš´ ì¸ìƒ"],
            "detailed_analysis": "ì—¬ë¦„ ì¿¨í†¤ íƒ€ì…ì¸ ë‹¹ì‹ ì€ ì°¨ê°€ìš´ ê³„ì—´ì˜ ë¶€ë“œëŸ¬ìš´ ìƒ‰ìƒì´ ê°€ì¥ ì˜ ì–´ìš¸ë¦¬ëŠ” ìš°ì•„í•œ íƒ€ì…ì…ë‹ˆë‹¤.\n\në‹¹ì‹ ì˜ í”¼ë¶€í†¤ì—ëŠ” ë¡œì¦ˆ, ë¼ë²¤ë”, ë¯¼íŠ¸, ìŠ¤ì¹´ì´ë¸”ë£¨ ë“±ì˜ íŒŒìŠ¤í…” ê³„ì—´ ìƒ‰ìƒì´ ì™„ë²½í•˜ê²Œ ì¡°í™”ë¥¼ ì´ë£¹ë‹ˆë‹¤. ë©”ì´í¬ì—… ì‹œì—ëŠ” ë„ˆë¬´ ê°•ë ¬í•˜ê±°ë‚˜ ë”°ëœ»í•œ í†¤ë³´ë‹¤ëŠ” ì¿¨í•˜ê³  ë¶€ë“œëŸ¬ìš´ ìƒ‰ìƒì„ ì„ íƒí•˜ì‹œë©´ ìì—°ìŠ¤ëŸ½ê²Œ ì„¸ë ¨ëœ ë¶„ìœ„ê¸°ë¥¼ ì—°ì¶œí•  ìˆ˜ ìˆì–´ìš”.\n\nì˜ìƒ ì„ íƒ ì‹œì—ë„ í™”ì´íŠ¸, ì‹¤ë²„, ë„¤ì´ë¹„, ê·¸ë ˆì´ ê³„ì—´ì„ ê¸°ë³¸ìœ¼ë¡œ í•˜ì—¬ í¬ì¸íŠ¸ ìƒ‰ìƒìœ¼ë¡œ íŒŒìŠ¤í…” í†¤ì„ í™œìš©í•˜ì‹œë©´ ìš°ì•„í•˜ë©´ì„œë„ í˜„ëŒ€ì ì¸ ë§¤ë ¥ì„ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        },
        "ê°€ì„": {
            "emotional_description": "ê¹Šì´ ìˆê³  ì„¸ë ¨ëœ ë‹¹ì‹ ì€ ê°€ì„ ì›œí†¤ íƒ€ì…ì…ë‹ˆë‹¤! ì§„í•˜ê³  ë”°ëœ»í•œ ìƒ‰ìƒì´ ë‹¹ì‹ ì˜ ì„±ìˆ™í•œ ë§¤ë ¥ì„ ì™„ë²½í•˜ê²Œ í‘œí˜„í•´ì¤ë‹ˆë‹¤.",
            "color_palette": ["#D2691E", "#CD853F", "#DEB887", "#BC8F8F", "#F4A460"],
            "style_keywords": ["ê¹Šì€", "ì„¸ë ¨ëœ", "ë”°ëœ»í•œ", "ì„±ìˆ™í•œ", "í´ë˜ì‹"],
            "makeup_tips": ["ë¸Œë¼ìš´ ê³„ì—´ ë¦½ìœ¼ë¡œ ì§€ì ì¸ ì¸ìƒ", "ê³¨ë“œ ë¸Œë¡ ì¦ˆ ì•„ì´ì„€ë„ë¡œ ê¹Šì€ ëˆˆë§¤", "ë”°ëœ»í•œ ì˜¤ë Œì§€ ë¸”ëŸ¬ì…”", "ë‹¤í¬ ë¸Œë¼ìš´ ë§ˆìŠ¤ì¹´ë¼ë¡œ ê°•ì¡°ëœ ì†ëˆˆì¹"],
            "detailed_analysis": "ê°€ì„ ì›œí†¤ íƒ€ì…ì¸ ë‹¹ì‹ ì€ ê¹Šì´ ìˆê³  í’ë¶€í•œ ìƒ‰ìƒì´ ê°€ì¥ ì˜ ì–´ìš¸ë¦¬ëŠ” ì„±ìˆ™í•˜ê³  ì„¸ë ¨ëœ íƒ€ì…ì…ë‹ˆë‹¤.\n\në‹¹ì‹ ì˜ í”¼ë¶€í†¤ì—ëŠ” ë¨¸ìŠ¤íƒ€ë“œ, ë¸Œë¦­, ì˜¬ë¦¬ë¸Œ, ë²„ê±´ë”” ë“±ì˜ ê¹Šê³  ë”°ëœ»í•œ ìƒ‰ìƒë“¤ì´ ìì—°ìŠ¤ëŸ½ê²Œ ì¡°í™”ë¥¼ ì´ë£¹ë‹ˆë‹¤. ë©”ì´í¬ì—…ì—ì„œëŠ” ë² ì´ì§€, ë¸Œë¼ìš´, ê³¨ë“œ ê³„ì—´ì„ í™œìš©í•˜ì—¬ ìì—°ìŠ¤ëŸ¬ìš°ë©´ì„œë„ ì„¸ë ¨ëœ ë¶„ìœ„ê¸°ë¥¼ ì—°ì¶œí•  ìˆ˜ ìˆì–´ìš”.\n\níŒ¨ì…˜ì—ì„œëŠ” ì¹´ë©œ, ë² ì´ì§€, ë¸Œë¼ìš´, ì™€ì¸ ì»¬ëŸ¬ ë“±ì„ ê¸°ë³¸ìœ¼ë¡œ í•˜ì—¬ í¬ì¸íŠ¸ ìƒ‰ìƒìœ¼ë¡œ ë¨¸ìŠ¤íƒ€ë“œë‚˜ ì˜¬ë¦¬ë¸Œ ê·¸ë¦°ì„ í™œìš©í•˜ì‹œë©´ í´ë˜ì‹í•˜ë©´ì„œë„ íŠ¸ë Œë””í•œ ìŠ¤íƒ€ì¼ì„ ì™„ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        },
        "ê²¨ìš¸": {
            "emotional_description": "ëª…í™•í•˜ê³  ê°•ë ¬í•œ ë‹¹ì‹ ì€ ê²¨ìš¸ ì¿¨í†¤ íƒ€ì…ì…ë‹ˆë‹¤! ì„ ëª…í•˜ê³  ë“œë¼ë§ˆí‹±í•œ ìƒ‰ìƒì´ ë‹¹ì‹ ì˜ ì¹´ë¦¬ìŠ¤ë§ˆë¥¼ í•œì¸µ ë” ë‹ë³´ì´ê²Œ í•©ë‹ˆë‹¤.",
            "color_palette": ["#FF1493", "#4169E1", "#000000", "#FFFFFF", "#8A2BE2"],
            "style_keywords": ["ëª…í™•í•œ", "ê°•ë ¬í•œ", "ì„ ëª…í•œ", "ë“œë¼ë§ˆí‹±", "ëª¨ë˜"],
            "makeup_tips": ["ë ˆë“œ ë¦½ìŠ¤í‹±ìœ¼ë¡œ ê°•ë ¬í•œ í¬ì¸íŠ¸", "ì‹¤ë²„ ì•„ì´ì„€ë„ë¡œ ì‹ ë¹„ë¡œìš´ ëˆˆë§¤", "ë¸”ë™ ì•„ì´ë¼ì´ë„ˆë¡œ ë˜ë ·í•œ ëˆˆë§¤", "ë³¼ë“œí•œ ì»¨íˆ¬ì–´ë§ìœ¼ë¡œ ì…ì²´ê°"],
            "detailed_analysis": "ê²¨ìš¸ ì¿¨í†¤ íƒ€ì…ì¸ ë‹¹ì‹ ì€ ì„ ëª…í•˜ê³  ê°•ë ¬í•œ ìƒ‰ìƒì´ ê°€ì¥ ì˜ ì–´ìš¸ë¦¬ëŠ” ë“œë¼ë§ˆí‹±í•˜ê³  ëª¨ë˜í•œ íƒ€ì…ì…ë‹ˆë‹¤.\n\në‹¹ì‹ ì˜ í”¼ë¶€í†¤ì—ëŠ” í“¨ì–´ í™”ì´íŠ¸, ë¸”ë™, ë¡œì–„ ë¸”ë£¨, ì—ë©”ë„ë“œ ê·¸ë¦° ë“±ì˜ ì„ ëª…í•˜ê³  ì°¨ê°€ìš´ ìƒ‰ìƒë“¤ì´ ì™„ë²½í•˜ê²Œ ì–´ìš¸ë¦½ë‹ˆë‹¤. ë©”ì´í¬ì—…ì—ì„œëŠ” ëª…í™•í•œ ì»¬ëŸ¬ ëŒ€ë¹„ë¥¼ í™œìš©í•˜ì—¬ ì‹œí¬í•˜ê³  ì„¸ë ¨ëœ ì´ë¯¸ì§€ë¥¼ ì—°ì¶œí•  ìˆ˜ ìˆì–´ìš”.\n\nì˜ìƒ ì„ íƒ ì‹œì—ë„ ë¸”ë™, í™”ì´íŠ¸, ê·¸ë ˆì´ë¥¼ ë² ì´ìŠ¤ë¡œ í•˜ì—¬ í¬ì¸íŠ¸ ìƒ‰ìƒìœ¼ë¡œ ë¹„ë¹„ë“œí•œ ì»¬ëŸ¬ë¥¼ í™œìš©í•˜ì‹œë©´ ë‹¹ì‹ ë§Œì˜ ë…íŠ¹í•˜ê³  ê°•ì¸í•œ ë§¤ë ¥ì„ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        }
    }
    
    return default_data.get(season, default_data["ë´„"])

def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()


def generate_welcome(db: Session, current_user: models.User, influencer_id: str | None = None):
    """
    Simple welcome endpoint used by frontend to provide a server-side welcome message
    and an optional influencer suggestion. This is intentionally lightweight so the
    frontend can fall back to local text if unavailable.
    """
    try:
        user_nick = getattr(current_user, 'nickname', None) or 'ì‚¬ìš©ì'
    except Exception:
        user_nick = 'ì‚¬ìš©ì'

    has_prev = False
    prev_summary = None
    try:
        if current_user and getattr(current_user, 'id', None):
            prev = (
                db.query(models.SurveyResult)
                .filter(models.SurveyResult.user_id == current_user.id, models.SurveyResult.is_active == True)
                .order_by(models.SurveyResult.created_at.desc())
                .first()
            )
            if prev:
                has_prev = True
                prev_summary = getattr(prev, 'result_name', None) or getattr(prev, 'result_tone', None)
    except Exception:
        # silently ignore DB failures here; frontend has a local fallback
        has_prev = False


    # Build a contextual welcome message using the LLM when possible.
    # If we have a previous diagnosis, ask the LLM to mention it; otherwise ask gentle diagnostic questions.
    try:
        infl_name = None
        infl_excerpt = None
        persona_notes = None

        # If caller provided an influencer id or slug, try to resolve it
        # to a full profile via the influencer service (or fallback list).
        if influencer_id:
            try:
                profiles = None
                if influencer_service and hasattr(influencer_service, 'influencer_profiles'):
                    res = influencer_service.influencer_profiles()
                    if isinstance(res, list):
                        outp = []
                        for it in res:
                            try:
                                if hasattr(it, 'dict'):
                                    outp.append(it.dict())
                                else:
                                    outp.append(it)
                            except Exception:
                                outp.append(it)
                        profiles = outp
                    else:
                        profiles = res
                if not profiles:
                    profiles = [
                        {'id': 'won_jun', 'name': 'ì›ì¤€', 'short_description': 'ì¹œê·¼í•˜ë©´ì„œë„ ì†”ì§í•œ ë¦¬ë·°', 'example_sentences': ['ì•ˆë…•í•˜ì„¸ìš” ê·€ìš¤ì´ë‹˜! ì›ì¤€ì…ë‹ˆë‹¤!']},
                        {'id': 'se_hyun', 'name': 'ì„¸í˜„', 'short_description': 'ìì—°ìŠ¤ëŸ¬ìš´ ë°ì¼ë¦¬ ë©”ì´í¬ì—… ì „ë¬¸', 'example_sentences': ['ì•ˆë…•í•˜ì„¸ìš” í¬ë“œë˜ê³¤ë‹˜! ì„¸í˜„ì´ì˜ˆìš”!']},
                        {'id': 'jong_min', 'name': 'ì¢…ë¯¼', 'short_description': 'ê°€ì„±ë¹„ ì¤‘ì‹¬ì˜ ì‹¤ìš©ì  ë¦¬ë·°', 'example_sentences': ['ì•ˆë…•í•˜ì„¸ìš” íŠ¸ë£¨ë“œë˜ê³¤ë‹˜! ì¢…ë¯¼ì…ë‹ˆë‹¤!']},
                        {'id': 'hye_kyung', 'name': 'í˜œê²½', 'short_description': 'ì¢…í•© ë·°í‹° ê°€ì´ë“œ', 'example_sentences': ['ì•ˆë…•í•˜ì„¸ìš” ë·°í‹°íŒ¨ë°€ë¦¬ë‹˜! í˜œê²½ì…ë‹ˆë‹¤!']},
                    ]

                # try match by id or name (case-insensitive)
                found = None
                for p in profiles:
                    try:
                        pid = str(p.get('id') or p.get('influencer_id') or '')
                        name = str(p.get('name') or p.get('short_name') or '')
                        if pid and pid == str(influencer_id):
                            found = p
                            break
                        if name and name.lower() == str(influencer_id).lower():
                            found = p
                            break
                    except Exception:
                        continue

                if found:
                    infl_name = found.get('name') or infl_name
                    infl_excerpt = found.get('short_description') or (found.get('example_sentences') and found.get('example_sentences')[0])
                    persona_notes = found.get('characteristics') or found.get('description') or None
            except Exception:
                pass

        # Build system + user prompt for the LLM
        system_prompt = "ë‹¹ì‹ ì€ í¼ìŠ¤ë„ì»¬ëŸ¬ ë¶„ì•¼ì˜ ì¹œì ˆí•œ ìƒë‹´ìì´ë©°, ì£¼ì–´ì§„ ì¸í”Œë£¨ì–¸ì„œ í˜ë¥´ì†Œë‚˜ì˜ ë§íˆ¬ì™€ ìŠ¤íƒ€ì¼ì„ ëª¨ë°©í•˜ì—¬ í•œêµ­ì–´ë¡œ ìì—°ìŠ¤ëŸ½ê³  ì¹œê·¼í•œ í™˜ì˜ ì¸ì‚¬ë¥¼ ì‘ì„±í•©ë‹ˆë‹¤. ì‘ë‹µì€ ì‚¬ìš©ìì—ê²Œ ë°”ë¡œ í‘œì‹œí•  í…ìŠ¤íŠ¸ í•œ ë©ì–´ë¦¬(ë¬¸ë‹¨)ë¡œë§Œ ì¶œë ¥í•˜ì„¸ìš”."

        user_prompt_lines = []
        if infl_name:
            user_prompt_lines.append(f"í˜ë¥´ì†Œë‚˜ ì´ë¦„: {infl_name}")
        if infl_excerpt:
            user_prompt_lines.append(f"ê°„ë‹¨ ì†Œê°œ: {infl_excerpt}")
        if persona_notes:
            user_prompt_lines.append(f"ë§íˆ¬ íŒíŠ¸: {persona_notes}")

        # Mandatory instruction: Request image upload
        user_prompt_lines.append("í•„ìˆ˜ í¬í•¨ ë‚´ìš©: ì •í™•í•œ í¼ìŠ¤ë„ì»¬ëŸ¬ ì§„ë‹¨ì„ ìœ„í•´ ì‚¬ìš©ìì˜ ì–¼êµ´ì´ ì˜ ë‚˜ì˜¨ ì‚¬ì§„(ì´ë¯¸ì§€)ì„ ì—…ë¡œë“œí•´ë‹¬ë¼ê³  ìš”ì²­í•˜ëŠ” ë¬¸ì¥ì„ ë°˜ë“œì‹œ í¬í•¨í•˜ì„¸ìš”.")

        if has_prev and prev_summary:
            user_prompt_lines.append(f"ì´ ì‚¬ìš©ìëŠ” ì´ì „ì— '{prev_summary}' íƒ€ì…ìœ¼ë¡œ ì§„ë‹¨ëœ ê¸°ë¡ì´ ìˆìŠµë‹ˆë‹¤. í™˜ì˜ ì¸ì‚¬ì—ì„œ 'ì´ì „ ì§„ë‹¨ ë‚´ì—­'ì´ë¼ëŠ” ë‹¨ì–´ë¥¼ í¬í•¨í•˜ì—¬ ì´ë¥¼ ì–¸ê¸‰í•˜ê³ , ì´ì „ ê²°ê³¼ë¥¼ ì°¸ê³ í•´ ì–´ë–¤ ë„ì›€ì„ ì¤„ ìˆ˜ ìˆëŠ”ì§€ ì•Œë ¤ì£¼ì„¸ìš”. ì¸í”Œë£¨ì–¸ì„œì˜ ë§íˆ¬ë¡œ ì‘ì„±í•˜ì„¸ìš”.")
        else:
            user_prompt_lines.append("ì´ ì‚¬ìš©ìëŠ” ì´ì „ ì§„ë‹¨ ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤. ìì—°ìŠ¤ëŸ½ê²Œ í¼ìŠ¤ë„ì»¬ëŸ¬ ì§„ë‹¨ì„ ì‹œì‘í•  ìˆ˜ ìˆë„ë¡ ì•ˆë‚´í•˜ê³ , ì¸í”Œë£¨ì–¸ì„œì˜ ë§íˆ¬ë¡œ ì‘ì„±í•˜ì„¸ìš”.")

        user_prompt_lines.append("ì‘ë‹µì€ 2~4ê°œì˜ ì§§ì€ ë¬¸ë‹¨(ë˜ëŠ” ë¬¸ì¥ë“¤)ìœ¼ë¡œ ìš”ì•½í•´ì£¼ê³ , ì¶”ê°€ ì§€ì‹œë‚˜ ë©”íƒ€ ì •ë³´ëŠ” ì¶œë ¥í•˜ì§€ ë§ˆì„¸ìš”. ì˜¤ì§ í™˜ì˜ í…ìŠ¤íŠ¸ë§Œ ì¶œë ¥í•˜ì„¸ìš”.")

        user_prompt = "\n".join(user_prompt_lines)

        # Call LLM
        try:
            resp = client.chat.completions.create(
                model=get_model_to_use(),
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt},
                ],
                max_tokens=250,
                temperature=0.7,
            )
            ai_message = resp.choices[0].message.content.strip()
            message = ai_message
        except Exception as e:
            # LLM failed â€” fall back to safe messages
            print(f"[welcome] LLM í˜¸ì¶œ ì‹¤íŒ¨, í´ë°± ë©”ì‹œì§€ ì‚¬ìš©: {e}")
            if has_prev and prev_summary:
                if infl_name:
                    message = f"ì•ˆë…•í•˜ì„¸ìš”, {user_nick}! ì´ì „ ì§„ë‹¨ì€ \"{prev_summary}\" íƒ€ì…ì…ë‹ˆë‹¤. {infl_name}ë‹˜ ìŠ¤íƒ€ì¼ì„ ì°¸ê³ í•´ ì´ì „ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë„ì™€ë“œë¦´ê²Œìš”. ì›í•˜ì‹œë©´ ë°”ë¡œ ì¶”ì²œì„ ì‹œì‘í• ê²Œìš”."
                else:
                    message = f"ì•ˆë…•í•˜ì„¸ìš”, {user_nick}! ì´ì „ ì§„ë‹¨ì€ \"{prev_summary}\" íƒ€ì…ì…ë‹ˆë‹¤. ì´ì „ ê²°ê³¼ë¥¼ ì°¸ê³ í•´ ë„ì›€ì„ ë“œë¦´ê²Œìš”. ë¬´ì—‡ì„ ë¨¼ì € ë„ì™€ë“œë¦´ê¹Œìš”?"
            else:
                if infl_name:
                    if infl_excerpt:
                        message = (
                            f"ì•ˆë…•í•˜ì„¸ìš”, {user_nick}! {infl_name}ë‹˜ ìŠ¤íƒ€ì¼ë¡œ í¼ìŠ¤ë„ì»¬ëŸ¬ë¥¼ ë„ì™€ë“œë¦´ê²Œìš” â€” {infl_excerpt} ì „ë¬¸ê°€ì…ë‹ˆë‹¤. "
                            "ë¨¼ì € ëª‡ ê°€ì§€ ì§ˆë¬¸ ë“œë¦´ê²Œìš”: í‰ì†Œ ìì£¼ ì…ëŠ” ì˜· ìƒ‰ìƒì€ ë¬´ì—‡ì¸ê°€ìš”? í”¼ë¶€í†¤ì€ ë°ì€ í¸ì¸ê°€ìš”, ì–´ë‘ìš´ í¸ì¸ê°€ìš”? í‰ì†Œ ì„ í˜¸í•˜ëŠ” ë©”ì´í¬ì—… ìŠ¤íƒ€ì¼ì€ ì–´ë–¤ê°€ìš”?"
                        )
                    else:
                        message = (
                            f"ì•ˆë…•í•˜ì„¸ìš”, {user_nick}! {infl_name}ë‹˜ ìŠ¤íƒ€ì¼ë¡œ í¼ìŠ¤ë„ì»¬ëŸ¬ ì§„ë‹¨ì„ ë„ì™€ë“œë¦´ê²Œìš”. "
                            "ë¨¼ì € ê°„ë‹¨í•œ ì§ˆë¬¸ ëª‡ ê°œë§Œ ë“œë¦´ê²Œìš”: í‰ì†Œ ìì£¼ ì…ëŠ” ìƒ‰ìƒì€ìš”? í”¼ë¶€í†¤ì€ ë°ì€ í¸ì¸ê°€ìš”, ì–´ë‘ìš´ í¸ì¸ê°€ìš”? ë©”ì´í¬ì—…ì´ë‚˜ ìŠ¤íƒ€ì¼ ì„ í˜¸ê°€ ìˆìœ¼ì‹ ê°€ìš”?"
                        )
                else:
                    message = (
                        f"ì•ˆë…•í•˜ì„¸ìš”, {user_nick}! ğŸ˜Š í¼ìŠ¤ë„ì»¬ëŸ¬ ì „ë¬¸ AI ì»¨ì„¤í„´íŠ¸ì…ë‹ˆë‹¤. "
                        "í¼ìŠ¤ë„ì»¬ëŸ¬ë¥¼ ì•Œì•„ë³´ë ¤ë©´ ê°„ë‹¨í•œ ì§ˆë¬¸ ëª‡ ê°€ì§€ê°€ í•„ìš”í•´ìš” â€” í‰ì†Œ ìì£¼ ì…ëŠ” ìƒ‰ìƒ, í”¼ë¶€í†¤(ë°ìŒ/ì–´ë‘ì›€), ì„ í˜¸í•˜ëŠ” ë©”ì´í¬ì—… ìŠ¤íƒ€ì¼ì„ ì•Œë ¤ì£¼ì‹¤ë˜ìš”?"
                    )
    except Exception as e:
        print(f"[welcome] ë©”ì‹œì§€ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
        message = f"ì•ˆë…•í•˜ì„¸ìš”, {user_nick}! ğŸ˜Š í¼ìŠ¤ë„ì»¬ëŸ¬ ì „ë¬¸ AI ì»¨ì„¤í„´íŠ¸ì…ë‹ˆë‹¤! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"

    return {"message": message, "has_previous": has_prev, "previous_summary": prev_summary}


@router.get('/influencer/profiles')
def get_influencer_profiles(db: Session = Depends(get_db), current_user: models.User = Depends(get_current_user)):
    """
    Proxy endpoint: returns influencer profiles for the frontend.
    If the `services.api_influencer` module is available, call its `influencer_profiles()` function.
    Otherwise return a safe fallback list.
    """
    try:
        profiles = None
        if influencer_service and hasattr(influencer_service, 'influencer_profiles'):
            res = influencer_service.influencer_profiles()
            # convert pydantic models to dicts when necessary
            if isinstance(res, list):
                out = []
                for it in res:
                    try:
                        if hasattr(it, 'dict'):
                            out.append(it.dict())
                        else:
                            out.append(it)
                    except Exception:
                        out.append(it)
                profiles = out
            else:
                profiles = res
        # fallback safe list if service not available
        if not profiles:
            profiles = [
                {'name': 'ì›ì¤€', 'short_description': 'ì¹œê·¼í•˜ë©´ì„œë„ ì†”ì§í•œ ë¦¬ë·°', 'example_sentences': ['ì•ˆë…•í•˜ì„¸ìš” ê·€ìš¤ì´ë‹˜! ì›ì¤€ì…ë‹ˆë‹¤!']},
                {'name': 'ì„¸í˜„', 'short_description': 'ìì—°ìŠ¤ëŸ¬ìš´ ë°ì¼ë¦¬ ë©”ì´í¬ì—… ì „ë¬¸', 'example_sentences': ['ì•ˆë…•í•˜ì„¸ìš” í¬ë“œë˜ê³¤ë‹˜! ì„¸í˜„ì´ì˜ˆìš”!']},
                {'name': 'ì¢…ë¯¼', 'short_description': 'ê°€ì„±ë¹„ ì¤‘ì‹¬ì˜ ì‹¤ìš©ì  ë¦¬ë·°', 'example_sentences': ['ì•ˆë…•í•˜ì„¸ìš” íŠ¸ë£¨ë“œë˜ê³¤ë‹˜! ì¢…ë¯¼ì…ë‹ˆë‹¤!']},
                {'name': 'í˜œê²½', 'short_description': 'ì¢…í•© ë·°í‹° ê°€ì´ë“œ', 'example_sentences': ['ì•ˆë…•í•˜ì„¸ìš” ë·°í‹°íŒ¨ë°€ë¦¬ë‹˜! í˜œê²½ì…ë‹ˆë‹¤!']},
            ]

        # Ensure each profile has a stable unique id (slug) for client-side linking
        def make_id(name: str) -> str:
            try:
                s = name.strip().lower()
                s = s.replace(' ', '_')
                import re
                s = re.sub(r'[^a-z0-9_\-]', '', s)
                return s
            except Exception:
                return str(name)

        for p in profiles:
            try:
                if isinstance(p, dict) and not p.get('id'):
                    nm = p.get('name') or p.get('short_name') or p.get('short_description') or 'unknown'
                    p['id'] = make_id(str(nm))
            except Exception:
                p['id'] = p.get('name') or 'unknown'


        return profiles
    except Exception as e:
        print(f"[get_influencer_profiles] proxy call failed: {e}")
        return []

# RAG ì¸ë±ìŠ¤ êµ¬ì¶• (ì„œë²„ ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ ì‹¤í–‰)
fixed_index = build_rag_index(client, "data/RAG/personal_color_RAG.txt")
trend_index = build_rag_index(client, "data/RAG/beauty_trend_2025_autumn_RAG.txt")

def clean_analysis_text(text: str) -> str:
    """
    ë¶„ì„ í…ìŠ¤íŠ¸ë¥¼ ì •ë¦¬í•˜ëŠ” í•¨ìˆ˜
    """
    if not text:
        return ""
    
    # ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°
    text = text.strip()
    
    # ì—°ì†ëœ ì¤„ë°”ê¿ˆì„ í•˜ë‚˜ë¡œ ì •ë¦¬
    import re
    text = re.sub(r'\n\s*\n\s*\n', '\n\n', text)
    
    # ì¤‘ë³µëœ ë¬¸ì¥ ì œê±° (ê°„ë‹¨í•œ ì¤‘ë³µ ì²´í¬)
    sentences = text.split('. ')
    unique_sentences = []
    seen = set()
    
    for sentence in sentences:
        sentence = sentence.strip()
        if sentence and sentence not in seen and len(sentence) > 10:
            seen.add(sentence)
            unique_sentences.append(sentence)
    
    return '. '.join(unique_sentences) if unique_sentences else text

async def save_chatbot_analysis_result(
    user_id: int,
    chat_history_id: int,
    db: Session,
    force: bool = False,
):
    """
    ğŸ†• ìƒˆë¡œìš´ í¼ìŠ¤ë„ ì»¬ëŸ¬ ì§„ë‹¨ ê¸°ë¡ ìƒì„± ğŸ†•
    
    âš ï¸ ì¤‘ìš”: ì´ í•¨ìˆ˜ëŠ” ìƒˆë¡œìš´ ì§„ë‹¨ ê¸°ë¡(SurveyResult)ì„ ìƒì„±í•©ë‹ˆë‹¤!
    - ì±—ë´‡ ëŒ€í™” ë¶„ì„ì„ í†µí•œ ìƒˆë¡œìš´ í¼ìŠ¤ë„ ì»¬ëŸ¬ ì§„ë‹¨
    - ë§ˆì´í˜ì´ì§€ ì§„ë‹¨ ê¸°ë¡ì— ìƒˆë¡œìš´ í•­ëª©ì´ ì¶”ê°€ë¨
    - ëŒ€í™” ë‚´ìš©ì„ AIê°€ ë¶„ì„í•˜ì—¬ ìƒˆë¡œìš´ ì§„ë‹¨ ê²°ê³¼ ë„ì¶œ
    
    í˜¸ì¶œ ì‹œì :
    1. ëŒ€í™” ì„¸ì…˜ ì¢…ë£Œ ì‹œ (ì¶©ë¶„í•œ ëŒ€í™”ê°€ ì§„í–‰ëœ ê²½ìš°)
    """
    try:
        # ğŸ” ì¤‘ë³µ ë°©ì§€: force=Trueì´ë©´ ì¤‘ë³µ ì²´í¬ë¥¼ ë¬´ì‹œí•˜ê³  í•­ìƒ ìƒˆ ë ˆì½”ë“œ ìƒì„±
        if not force:
            existing_result = db.query(models.SurveyResult).filter(
                models.SurveyResult.user_id == user_id,
                models.SurveyResult.source_type == "chatbot",
                models.SurveyResult.is_active == True
            ).order_by(models.SurveyResult.created_at.desc()).first()

            # ìµœê·¼ ìƒì„±ëœ ì§„ë‹¨ ê²°ê³¼ê°€ 5ë¶„ ì´ë‚´ë¼ë©´ ì¤‘ë³µìœ¼ë¡œ ê°„ì£¼
            if existing_result:
                from datetime import timedelta
                # DBì— ì €ì¥ëœ created_atì´ tz-naiveì¸ ê²½ìš°ê°€ ìˆì–´ subtraction ì—ëŸ¬ê°€ ë‚  ìˆ˜ ìˆìŒ
                existing_created_at = existing_result.created_at
                if existing_created_at is None:
                    # ì•ˆì „í•˜ê²Œ ë„˜ì–´ê°
                    existing_created_at = datetime.now(timezone.utc)
                # if DB returned a naive datetime (no tzinfo), assume UTC
                if existing_created_at.tzinfo is None:
                    existing_created_at = existing_created_at.replace(tzinfo=timezone.utc)

                time_diff = datetime.now(timezone.utc) - existing_created_at
                if time_diff < timedelta(minutes=5):
                    print(f"ğŸ”„ ì¤‘ë³µ ì§„ë‹¨ ë°©ì§€: ìµœê·¼ {time_diff.seconds}ì´ˆ ì „ì— ìƒì„±ëœ ê²°ê³¼ ì¬ì‚¬ìš©")
                    print(f"   - ê¸°ì¡´ ê²°ê³¼ ID: {existing_result.id}")
                    print(f"   - ê¸°ì¡´ ê²°ê³¼ íƒ€ì…: {existing_result.result_tone}")
                    return existing_result
        print(f"ğŸ” ìƒˆë¡œìš´ ì§„ë‹¨ ê¸°ë¡ ìƒì„± ì‹œì‘: user_id={user_id}, chat_history_id={chat_history_id}")
        
        # ëŒ€í™” íˆìŠ¤í† ë¦¬ì—ì„œ ë©”ì‹œì§€ë“¤ ê°€ì ¸ì˜¤ê¸°
        messages = db.query(models.ChatMessage).filter_by(
            history_id=chat_history_id
        ).order_by(models.ChatMessage.created_at.asc()).all()
        
        if not messages:
            print("âŒ ëŒ€í™” ë©”ì‹œì§€ê°€ ì—†ì–´ì„œ ì§„ë‹¨ ë¶ˆê°€")
            return None
            
        print(f"ğŸ“ ëŒ€í™” ë©”ì‹œì§€ {len(messages)}ê°œ ë°œê²¬, ë¶„ì„ ì‹œì‘...")
        
        # ëŒ€í™” ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ í¼ìŠ¤ë„ ì»¬ëŸ¬ ê²°ì •
        conversation_text = ""
        for msg in messages:
            if msg.role == "user":
                conversation_text += f"User: {msg.text}\n"
            elif msg.role == "ai":
                try:
                    ai_data = json.loads(msg.text)
                    conversation_text += f"AI: {ai_data.get('description', msg.text)}\n"
                except:
                    conversation_text += f"AI: {msg.text}\n"
        
        # ë¨¼ì € color serviceë¥¼ í˜¸ì¶œí•´ í¼ìŠ¤ë„ì»¬ëŸ¬ ê¸°ë°˜ í†¤ì„ ì–»ì–´ë³¸ë‹¤ (ìš°ì„ )
        primary_tone = None
        sub_tone = None
        try:
            if api_color_service:
                color_payload = api_color_service.ColorRequest(
                    user_text=conversation_text,
                    conversation_history=None,
                )
                color_resp = await api_color_service.analyze_color(color_payload)
                # color_resp may be a pydantic model
                hints = None
                if hasattr(color_resp, 'detected_color_hints'):
                    hints = color_resp.detected_color_hints
                elif isinstance(color_resp, dict):
                    hints = color_resp.get('detected_color_hints')
                if isinstance(hints, dict):
                    primary_tone = hints.get('primary_tone')
                    sub_tone = hints.get('sub_tone')
        except Exception as e:
            print(f"âš ï¸ color service call failed, falling back to heuristic: {e}")

        # ì»¬ëŸ¬ ê¸°ë°˜ í†¤ì´ ì—†ìœ¼ë©´ ê¸°ì¡´ ëŒ€í™” ê¸°ë°˜ íœ´ë¦¬ìŠ¤í‹±ìœ¼ë¡œ ë³´ì™„
        if not primary_tone or not sub_tone:
            primary_tone, sub_tone = analyze_conversation_for_color_tone(
                conversation_text, ""  # í˜„ì¬ ì§ˆë¬¸ì€ ë¹ˆ ë¬¸ìì—´ë¡œ ì²˜ë¦¬ (ì „ì²´ ëŒ€í™” ê¸°ë°˜ ë¶„ì„)
            )

        # Normalize tones into canonical values before proceeding
        try:
            primary_tone, sub_tone = normalize_personal_color(primary_tone, sub_tone)
        except Exception:
            pass

        print(f"ğŸ¨ AI ë¶„ì„ ê²°ê³¼: {primary_tone}í†¤ {sub_tone}")
        
        # ğŸ†• OpenAIë¥¼ í†µí•œ ì™„ì „í•œ ì§„ë‹¨ ë°ì´í„° ìƒì„±
        print("ğŸ¤– OpenAI APIë¥¼ í†µí•œ ë§ì¶¤í˜• ì§„ë‹¨ ë°ì´í„° ìƒì„± ì¤‘...")
        ai_diagnosis_data = generate_complete_diagnosis_data(conversation_text, sub_tone)
        
        # í…ìŠ¤íŠ¸ ì •ë¦¬
        cleaned_analysis = clean_analysis_text(ai_diagnosis_data["detailed_analysis"])
        
        # ê¸°ë³¸ íƒ€ì… ì •ë³´ì— AI ìƒì„± ë°ì´í„° ì ìš©
        type_info = {
            "name": f"{sub_tone} {primary_tone}í†¤",
            "description": ai_diagnosis_data["emotional_description"],
            "detailed_analysis": cleaned_analysis,
            "color_palette": ai_diagnosis_data["color_palette"],
            "style_keywords": ai_diagnosis_data["style_keywords"],
            "makeup_tips": ai_diagnosis_data["makeup_tips"]
        }
        
        # ê²°ê³¼ í†¤ ë° ì‹ ë¢°ë„ ì„¤ì •  
        result_tone = f"{primary_tone}í†¤ {sub_tone}"
        confidence = 0.85  # ê¸°ë³¸ ì‹ ë¢°ë„
        
        # primary_type ë§¤í•‘
        type_mapping = {
            ("ì›œ", "ë´„"): "spring",
            ("ì›œ", "ê°€ì„"): "autumn", 
            ("ì¿¨", "ì—¬ë¦„"): "summer",
            ("ì¿¨", "ê²¨ìš¸"): "winter"
        }
        primary_type = type_mapping.get((primary_tone, sub_tone), "spring")
        
        # Top types ìƒì„± (AI ìƒì„± ë°ì´í„° ê¸°ë°˜)
        top_types = [
            {
                "type": primary_type,
                "name": f"{sub_tone} {primary_tone}í†¤",
                "description": type_info["description"],
                "color_palette": type_info["color_palette"],
                "style_keywords": type_info["style_keywords"],
                "makeup_tips": type_info["makeup_tips"],
                "score": int(confidence * 100)
            }
        ]
        
        # SurveyResultë¡œ ìƒˆë¡œìš´ ì§„ë‹¨ ê¸°ë¡ ì €ì¥
        print(f"ğŸ’¾ ìƒˆë¡œìš´ ì§„ë‹¨ ê¸°ë¡ DB ì €ì¥ ì‹œì‘...")
        survey_result = models.SurveyResult(
            user_id=user_id,
            result_tone=primary_type,
            confidence=confidence,
            total_score=int(confidence * 100),
            source_type="chatbot",  # ì±—ë´‡ ë¶„ì„ ì¶œì²˜ í‘œì‹œ
            detailed_analysis=type_info["detailed_analysis"],
            result_name=type_info["name"],
            result_description=type_info["description"],
            color_palette=json.dumps(type_info["color_palette"], ensure_ascii=False),
            style_keywords=json.dumps(type_info["style_keywords"], ensure_ascii=False),
            makeup_tips=json.dumps(type_info["makeup_tips"], ensure_ascii=False),
            top_types=json.dumps(top_types, ensure_ascii=False)
        )
        
        db.add(survey_result)
        db.commit()
        db.refresh(survey_result)
        
        print(f"âœ… ìƒˆë¡œìš´ ì§„ë‹¨ ê¸°ë¡ ìƒì„± ì™„ë£Œ: survey_result_id={survey_result.id}")
        print(f"   - ì§„ë‹¨ íƒ€ì…: {survey_result.result_tone}")
        print(f"   - ì‹ ë¢°ë„: {survey_result.confidence}")
        print(f"   âš ï¸ ë§ˆì´í˜ì´ì§€ ì§„ë‹¨ ê¸°ë¡ì— ìƒˆë¡œìš´ í•­ëª© ì¶”ê°€ë¨")
        
        return survey_result
        
    except Exception as e:
        print(f"âŒ ì±—ë´‡ ë¶„ì„ ê²°ê³¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
        db.rollback()
        return None


@router.post("/report/save", response_model=ReportResponse)
async def save_report_now(
    request: ReportCreate,
    current_user: models.User = Depends(get_current_user),
    db: Session = Depends(get_db),
):
    """
    í”„ë¡ íŠ¸ì—”ë“œì—ì„œ 3í„´ë§ˆë‹¤ í˜¸ì¶œí•˜ëŠ” ì—”ë“œí¬ì¸íŠ¸ì…ë‹ˆë‹¤.
    force=Trueë¡œ `save_chatbot_analysis_result`ë¥¼ í˜¸ì¶œí•´ í•­ìƒ ìƒˆ ì§„ë‹¨ ê¸°ë¡ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    if not request.history_id:
        raise HTTPException(status_code=400, detail="history_idê°€ í•„ìš”í•©ë‹ˆë‹¤")

    survey_result = await save_chatbot_analysis_result(
        user_id=current_user.id,
        chat_history_id=request.history_id,
        db=db,
        force=request.force or True,  # ê¸°ë³¸ ë™ì‘ì€ ê°•ì œ ìƒì„±
    )

    if survey_result:
        # ìƒì„±ëœ survey_resultì˜ ìš”ì•½/ë¯¸ë¦¬ë³´ê¸° ë°ì´í„°ë¥¼ ìƒì„±
        try:
            from utils.report_generator import PersonalColorReportGenerator

            report_generator = PersonalColorReportGenerator()

            # survey_resultì— ì €ì¥ëœ JSON í•„ë“œ íŒŒì‹±
            def parse_json_field(val):
                if not val:
                    return []
                if isinstance(val, str):
                    try:
                        return json.loads(val)
                    except:
                        return []
                return val

            survey_data = {
                "result_tone": survey_result.result_tone,
                "result_name": survey_result.result_name,
                "confidence": survey_result.confidence,
                "detailed_analysis": survey_result.detailed_analysis,
                "color_palette": parse_json_field(survey_result.color_palette),
                "style_keywords": parse_json_field(survey_result.style_keywords),
                "makeup_tips": parse_json_field(survey_result.makeup_tips),
            }

            # ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¡°íšŒ
            chat_history = []
            try:
                messages = db.query(models.ChatMessage).filter_by(
                    history_id=request.history_id
                ).order_by(models.ChatMessage.created_at.asc()).all()
                chat_history = [
                    {"role": msg.role, "text": msg.text, "created_at": msg.created_at.isoformat()}
                    for msg in messages
                ]
            except Exception:
                chat_history = []

            report_data = report_generator.generate_report_data(survey_data, chat_history)

        except Exception as e:
            print(f"âš ï¸ ë¦¬í¬íŠ¸ ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            report_data = None

        # í”„ë¡ íŠ¸ê°€ ì¦‰ì‹œ í‘œì‹œí•˜ê¸° ì‰¬ìš´ ë¯¸ë¦¬ë³´ê¸° í•„ë“œë„ í•¨ê»˜ ë°˜í™˜
        return ReportResponse(
            survey_result_id=survey_result.id,
            message="ì§„ë‹¨ ê¸°ë¡ ìƒì„± ì™„ë£Œ",
            created_at=survey_result.created_at,
            result_tone=survey_result.result_tone,
            result_name=survey_result.result_name,
            detailed_analysis=survey_result.detailed_analysis,
            color_palette=(json.loads(survey_result.color_palette) if survey_result.color_palette else []),
            style_keywords=(json.loads(survey_result.style_keywords) if survey_result.style_keywords else []),
            makeup_tips=(json.loads(survey_result.makeup_tips) if survey_result.makeup_tips else []),
            report_data=report_data,
        )
    else:
        raise HTTPException(status_code=500, detail="ì§„ë‹¨ ê¸°ë¡ ìƒì„± ì‹¤íŒ¨")

def detect_emotion(text: str) -> str:
    """
    OpenAI ê¸°ë°˜ ê°ì • ë¶„ì„ (Lottie emotion string ë°˜í™˜)
    """
    prompt = f"""
ë‹¤ìŒ ì‚¬ìš©ì ë°œí™”ì˜ ê°ì •ì„ ì•„ë˜ ëª©ë¡ ì¤‘ í•˜ë‚˜ë¡œë§Œ ë¶„ë¥˜í•˜ì„¸ìš”. ë°˜ë“œì‹œ í•œ ë‹¨ì–´ë§Œ ë‹µí•˜ì„¸ìš”. ë‹¤ë¥¸ ë‹¨ì–´, ì„¤ëª… ì—†ì´.
ëª©ë¡: happy, sad, angry, love, fearful, neutral
ì˜ˆì‹œ:
ë°œí™”: "{text}"
ê°ì • (ëª©ë¡ ì¤‘ í•˜ë‚˜, í•œ ë‹¨ì–´ë§Œ):
"""
    prompt = f"""
ë‹¤ìŒ ì‚¬ìš©ì ë°œí™”ì˜ ê°ì •ì„ ì•„ë˜ ëª©ë¡ ì¤‘ í•˜ë‚˜ë¡œë§Œ ë¶„ë¥˜í•˜ì„¸ìš”. ë°˜ë“œì‹œ í•œ ë‹¨ì–´ë§Œ ë‹µí•˜ì„¸ìš”. ë‹¤ë¥¸ ë‹¨ì–´, ì„¤ëª… ì—†ì´.
ëª©ë¡: happy, sad, angry, love, fearful, neutral
ì˜ˆì‹œ (í•œêµ­ì–´ ë‹¤ì–‘í•œ í‘œí˜„ í¬í•¨):
- "ì˜¤ëŠ˜ ë„ˆë¬´ í˜ë“¤ì—ˆì–´ìš”" â†’ sad
- "ì •ë§ ê³ ë§ˆì›Œìš”!" â†’ happy
- "í™”ê°€ ë‚˜ìš”" â†’ angry
- "ë‚´ ë…¸ë ¥ì„ ë¬´ì‹œí•˜ëŠ” íƒœë„ì— ë¶„ë…¸ê°€ ì¹˜ë°€ì–´ìš”" â†’ angry
- "ê·¸ ì‚¬ëŒ íƒœë„ ë•Œë¬¸ì— ì—´ì´ ë°›ì•„ìš”" â†’ angry
- "ì‚¬ë‘í•´ìš”" â†’ love
- "ê·¸ì™€ í•¨ê»˜ ìˆìœ¼ë©´ í–‰ë³µí•˜ê³  ì‚¬ë‘ì„ ëŠê»´" â†’ love
- "ë¬´ì„œì›Œì„œ í˜¼ì ìˆì„ ìˆ˜ê°€ ì—†ì–´ìš”" â†’ fearful
- "ë†’ì€ ê³³ì— ì„œë©´ ë‹¤ë¦¬ê°€ ë–¨ë¦¬ê³  ë¬´ì„œì›Œìš”" â†’ fearful
- "ë³„ ê°ì •ì´ ì—†ì–´ìš”" â†’ neutral
ë°œí™”: "{text}"
ê°ì • (ëª©ë¡ ì¤‘ í•˜ë‚˜, í•œ ë‹¨ì–´ë§Œ):
"""
    try:
        response = client.chat.completions.create(
            model=get_model_to_use(),
            messages=[{"role": "system", "content": "ë„ˆëŠ” ê°ì • ë¶„ì„ ì „ë¬¸ê°€ì•¼. ë°˜ë“œì‹œ ëª©ë¡ ì¤‘ í•˜ë‚˜ì˜ ê°ì •ë§Œ í•œ ë‹¨ì–´ë¡œ ë‹µí•´ì¤˜."},
                      {"role": "user", "content": prompt}],
            max_tokens=5,
            temperature=0.0
        )
        emotion = response.choices[0].message.content.strip().lower()
        # ê°ì • ë‹¨ì–´ë§Œ ì¶”ì¶œ (ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ë‹¨ì–´ë§Œ ë°˜í™˜)
        valid_emotions = ["happy", "sad", "angry", "love", "fearful", "neutral"]
        for e in valid_emotions:
            if emotion == e:
                return e
        # í˜¹ì‹œ ì—¬ëŸ¬ ë‹¨ì–´ê°€ ì„ì—¬ ìˆìœ¼ë©´ ì²« ë²ˆì§¸ ìœ íš¨ ë‹¨ì–´ë§Œ ë°˜í™˜
        for e in valid_emotions:
            if e in emotion:
                return e
        return "neutral"
    except Exception as e:
        print(f"[detect_emotion] OpenAI ê°ì • ë¶„ì„ ì˜¤ë¥˜: {e}")
        return "neutral"


def _normalize_emotion_label(label: str) -> str:
    """Normalize arbitrary labels to the canonical set or return empty string."""
    if not label or not isinstance(label, str):
        return ""
    l = label.strip().lower()
    # emoji mapping: map common emoji characters to canonical labels
    emoji_map = {
        "ğŸ˜„": "happy",
        "ğŸ˜Š": "happy",
        "ğŸ™‚": "happy",
        "ğŸ˜": "happy",
        "ğŸ˜‚": "happy",
        "ğŸ˜­": "sad",
        "ğŸ˜¢": "sad",
        "ğŸ˜": "sad",
        "ğŸ˜ ": "angry",
        "ğŸ˜¡": "angry",
        "ğŸ’”": "sad",
        "ğŸ’–": "love",
        "â¤ï¸": "love",
        "ğŸ˜": "love",
        "ğŸ˜¨": "fearful",
        "ğŸ˜±": "fearful",
    }
    # if the label itself is an emoji or contains one, map it
    for emj, mapped in emoji_map.items():
        if emj == l or emj in label:
            return mapped
    # allowed canonical emotions
    valid = ["happy", "sad", "angry", "love", "fearful", "neutral"]
    # direct match
    if l in valid:
        return l
    # common synonyms mapping
    synonyms = {
        "joy": "happy",
        "happiness": "happy",
        "depressed": "sad",
        "anger": "angry",
        "fear": "fearful",
        "afraid": "fearful",
        "love": "love",
        "liked": "love",
    }
    if l in synonyms:
        return synonyms[l]
    # if label contains a valid token, pick first
    for v in valid:
        if v in l:
            return v
    return ""


def _precheck_strong_anger_fear(user_text: str, convo_text: str | None = None) -> str:
    """
    Lightweight pre-check for strong anger/fear lexical cues in Korean.
    Returns 'angry' or 'fearful' if a strong cue is found, otherwise empty string.
    """
    try:
        import re
        txt = (user_text or "") + "\n" + (convo_text or "")
        txt = txt.lower()
        # Anger cues (Korean stems)
        if re.search(r"(ì—´ì´ ë°›|ì—´ë°›|ë¶„ë…¸|í™”ê°€ ë‚˜|ì„±ëƒ„|ì§œì¦|ë¶„ê°œ|ê²©ë¶„|ì°¸ì„ ìˆ˜ ì—†)", txt):
            return 'angry'
        # Fear/anxiety cues
        if re.search(r"(ë¬´ì„œ|ë‘ë µ|ê³µí¬|ê²|ë¶ˆì•ˆ|ë§‰ë§‰|ìˆ¨ì´ ë§‰íˆ|ì˜¤ì‹¹)", txt):
            return 'fearful'
    except Exception:
        return ""
    return ""


async def _call_api_emotion_service(question: str, conversation_history: list | None = None):
    """Call the external api_emotion service if available and return the parsed response or None.

    Handles both coroutine and sync implementations by running sync calls in a thread executor.
    """
    if not api_emotion_service:
        return None
    try:
        # build payload if the service exposes the request model
        if hasattr(api_emotion_service, 'EmotionRequest'):
            payload = api_emotion_service.EmotionRequest(user_text=question, conversation_history=conversation_history)
        else:
            payload = {"user_text": question, "conversation_history": conversation_history}

        gen = getattr(api_emotion_service, 'generate_emotion', None)
        if gen is None:
            return None

        if asyncio.iscoroutinefunction(gen):
            resp = await gen(payload)
        else:
            # run sync function in executor to avoid blocking event loop
            loop = asyncio.get_running_loop()
            resp = await loop.run_in_executor(None, lambda: gen(payload))

        # convert pydantic model to dict if needed
        if hasattr(resp, 'dict'):
            return resp.dict()
        return resp if isinstance(resp, dict) else None
    except Exception as e:
        print(f"[analyze] api_emotion call failed: {e}")
        return None


def _extract_emotion_from_orchestrator(emotion_res: dict) -> str:
    """Try to extract a canonical emotion label from the orchestrator's parsed emotion dict."""
    if not emotion_res or not isinstance(emotion_res, dict):
        return ""
    # Prefer explicit canonical labels or primary tone fields returned by the model/orchestrator
    for key in ('canonical_label', 'canonical', 'primary_tone', 'primary', 'label', 'emotion'):
        val = emotion_res.get(key)
        if isinstance(val, str) and val:
            lab = _normalize_emotion_label(val)
            if lab:
                return lab

    # Next, prefer tone_tags (they often contain more descriptive tokens)
    tags = emotion_res.get('tone_tags') or emotion_res.get('tags')
    if tags and isinstance(tags, list):
        # Prefer explicit anger tokens if present (increase sensitivity)
        for t in tags:
            lab = _normalize_emotion_label(t)
            if lab == 'angry':
                return 'angry'
        for t in tags:
            lab = _normalize_emotion_label(t)
            if lab:
                return lab

    return ""


async def _resolve_emotion_tag(emotion_res: dict, conversation_history: list | None, question: str) -> str:
    """High-level resolver: orchestrator -> api_emotion -> local detector."""
    # 1) orchestrator
    try:
        val = _extract_emotion_from_orchestrator(emotion_res)
        if val:
            return val
    except Exception:
        pass

    # 2) external service
    try:
        api_resp = await _call_api_emotion_service(question, conversation_history)
        if isinstance(api_resp, dict):
            # Prefer explicit canonical_label from api_emotion if present
            canon_label = api_resp.get('canonical_label') or api_resp.get('canonical')
            if isinstance(canon_label, str) and canon_label:
                try:
                    return to_canonical(canon_label)
                except Exception:
                    return _normalize_emotion_label(canon_label) or ''
            # Prefer tone_tags (they often contain more specific tokens)
            tokens = api_resp.get('tone_tags') or api_resp.get('tags')
            if tokens:
                if isinstance(tokens, str):
                    tokens = [tokens]
                if isinstance(tokens, list):
                    # normalize all tokens then prefer 'angry' if any
                    canons = []
                    for t in tokens:
                        try:
                            canon = to_canonical(t)
                        except Exception:
                            canon = _normalize_emotion_label(t)
                        if canon:
                            canons.append(canon)
                    if 'angry' in canons:
                        return 'angry'
                    for canon in canons:
                        if canon and canon != 'neutral':
                            return canon

            # Try scanning description/summary for lexical cues (Korean stems included in SYNONYMS)
            desc = api_resp.get('description') or api_resp.get('summary') or ''
            if isinstance(desc, str) and desc:
                try:
                    desc_canon = to_canonical(desc)
                except Exception:
                    desc_canon = _normalize_emotion_label(desc)
                if desc_canon and desc_canon != 'neutral':
                    return desc_canon

            # Fallback to primary fields (canonicalize)
            for key in ('primary_tone', 'primary', 'label', 'tag', 'emotion'):
                v = api_resp.get(key)
                if isinstance(v, str):
                    try:
                        lab = to_canonical(v)
                    except Exception:
                        lab = _normalize_emotion_label(v)
                    if lab:
                        return lab
    except Exception:
        pass

    # 3) local fallback
    try:
        local = detect_emotion(question)
        local_norm = _normalize_emotion_label(local) or local
        if local_norm:
            return local_norm
    except Exception:
        pass

    return "neutral"

@router.post("/analyze", response_model=ChatbotHistoryResponse)
async def analyze(
    request: ChatbotRequest,
    current_user: models.User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    # Debug: log incoming request and user for tracing 400 errors
    try:
        print(f"[analyze] incoming request: history_id={request.history_id}, question={request.question}")
        print(f"[analyze] current_user.id={getattr(current_user,'id',None)}")
    except Exception:
        pass

    # ì‹ ê·œ ì„¸ì…˜ ìƒì„± ë˜ëŠ” ê¸°ì¡´ ì„¸ì…˜ ì´ì–´ë°›ê¸°
    if not request.history_id:
        chat_history = models.ChatHistory(user_id=current_user.id)
        db.add(chat_history)
        db.commit()
        db.refresh(chat_history)
    else:
        chat_history = db.query(models.ChatHistory).filter_by(id=request.history_id, user_id=current_user.id).first()
        if not chat_history:
            raise HTTPException(status_code=404, detail="í•´ë‹¹ history_id ì„¸ì…˜ ì—†ìŒ")
        if chat_history.ended_at:
            # Log ended session to help debugging
            print(f"[analyze] requested history_id {request.history_id} is already ended at {chat_history.ended_at}")
            raise HTTPException(status_code=400, detail="ì´ë¯¸ ì¢…ë£Œëœ ì„¸ì…˜ì…ë‹ˆë‹¤.")
    user_msg = models.ChatMessage(history_id=chat_history.id, role="user", text=request.question)
    db.add(user_msg)
    db.commit()
    db.refresh(user_msg)

    # If the incoming request has an empty question, treat this call as a "welcome" request
    # and return the same welcome message that `/welcome` provides so clients can use
    # a single endpoint for both welcome + normal analyze flows.
    if not request.question or (isinstance(request.question, str) and request.question.strip() == ""):
        try:
            # Reuse the existing welcome helper to build the message. Pass current db and user.
            infl_id = chat_history.influencer_id or chat_history.influencer_name
            welcome_resp = generate_welcome(db=db, current_user=current_user, influencer_id=infl_id)
            welcome_text = (welcome_resp or {}).get('message') or 'ì•ˆë…•í•˜ì„¸ìš”! í¼ìŠ¤ë„ì»¬ëŸ¬ AIì…ë‹ˆë‹¤.'
        except Exception as e:
            print(f"[analyze] welcome generation failed: {e}")
            welcome_text = 'ì•ˆë…•í•˜ì„¸ìš”! í¼ìŠ¤ë„ì»¬ëŸ¬ AIì…ë‹ˆë‹¤.'

        # persist AI welcome message
        ai_msg = models.ChatMessage(
            history_id=chat_history.id,
            role='ai',
            text=welcome_text,
            raw=json.dumps({'description': welcome_text}, ensure_ascii=False),
        )
        db.add(ai_msg)
        db.commit()
        db.refresh(ai_msg)

        # build items response compatible with frontend ChatbotHistoryResponse
        item = {
            'question_id': 0,
            'question': '',
            'answer': welcome_text,
            'chat_res': {
                'primary_tone': '',
                'sub_tone': '',
                'description': welcome_text,
                'recommendations': [],
                'emotion': 'neutral',
            }
        }
        # ensure top-level `emotion` exists to satisfy ChatItemModel response validation
        item['emotion'] = item['chat_res'].get('emotion', 'neutral')
        return {'history_id': chat_history.id, 'items': [item]}
    # ì´ì „ ëŒ€í™” íˆìŠ¤í† ë¦¬ì—ì„œ ì‚¬ìš©ì ì •ë³´ ìˆ˜ì§‘
    prev_messages = db.query(models.ChatMessage).filter_by(history_id=chat_history.id).order_by(models.ChatMessage.id.asc()).all()
    # ë‹‰ë„¤ì„ ì‚¬ìš©: current_user.nicknameì´ ìˆìœ¼ë©´, ì—†ìœ¼ë©´ 'ì‚¬ìš©ì'
    user_display_name = getattr(current_user, "nickname", None)
    if not user_display_name:
        user_display_name = "ì‚¬ìš©ì"
    # ìµœê·¼ ë©”ì‹œì§€ëŠ” later used to build `convo_list`; no separate summary needed here.
    
    # Use the local orchestrator service to run color+emotion -> influencer chain
    if not orchestrator_service:
        raise HTTPException(status_code=500, detail="Orchestrator service not available in this runtime")

    # Build a structured conversation history for the orchestrator
    convo_list = []
    for msg in prev_messages:
        try:
            if msg.role == 'user':
                convo_list.append({"role": "user", "text": msg.text})
            else:
                # ai messages may contain JSON with a description field
                try:
                    ai_data = json.loads(msg.text)
                    convo_list.append({"role": "ai", "text": ai_data.get("description", msg.text)})
                except Exception:
                    convo_list.append({"role": "ai", "text": msg.text})
        except Exception:
            continue

    try:
        # include any persona stored on the chat history so the orchestrator and influencer chain
        # can adapt responses to the selected persona
        persona_name = getattr(chat_history, 'influencer_name', None)
        orch_payload = orchestrator_service.OrchestratorRequest(
            user_text=request.question,
            conversation_history=convo_list,
            user_nickname=getattr(current_user, 'nickname', None),
            personal_color=None,
            use_color=True,
            use_emotion=True,
        )
        # attach influencer persona if available (some orchestrator implementations accept this)
        if persona_name and hasattr(orch_payload, 'dict'):
            # safest approach: set attribute when present
            try:
                setattr(orch_payload, 'influencer_name', persona_name)
            except Exception:
                pass
        orch_resp = await orchestrator_service.analyze(orch_payload)

        # Debug: print orchestrator full response for troubleshooting
        try:
            orch_serializable = None
            if hasattr(orch_resp, 'dict'):
                try:
                    orch_serializable = orch_resp.dict()
                except Exception:
                    # some pydantic models may require .dict(exclude_none=True)
                    try:
                        orch_serializable = orch_resp.dict(exclude_none=True)
                    except Exception:
                        orch_serializable = None
            elif isinstance(orch_resp, dict):
                orch_serializable = orch_resp

            if orch_serializable is not None:
                try:
                    print("[analyze] orch_resp:", json.dumps(orch_serializable, ensure_ascii=False)[:4000])
                except Exception:
                    print("[analyze] orch_resp (repr):", repr(orch_serializable)[:4000])
            else:
                print("[analyze] orch_resp (raw):", repr(orch_resp)[:4000])
        except Exception as e:
            print(f"[analyze] orch_resp logging failed: {e}")
    except Exception as e:
        print(f"âŒ Orchestrator error: {e}")
        raise HTTPException(status_code=500, detail=f"Orchestrator failed: {str(e)}")
    # Extract results (orchestrator now returns namespaced structures)
    raw_emotion = orch_resp.emotion if getattr(orch_resp, 'emotion', None) is not None else (orch_resp.get('emotion') if isinstance(orch_resp, dict) else {})
    raw_color = orch_resp.color if getattr(orch_resp, 'color', None) is not None else (orch_resp.get('color') if isinstance(orch_resp, dict) else {})

    # unwrap parsed parts if present
    def _unwrap(parsed_like):
        if isinstance(parsed_like, dict) and parsed_like.get("parsed") is not None:
            return parsed_like.get("parsed"), parsed_like
        return (parsed_like if isinstance(parsed_like, dict) else {}, parsed_like)

    emotion_res, emotion_wrapped = _unwrap(raw_emotion)
    color_res, color_wrapped = _unwrap(raw_color)

    # Prefer influencer-styled text when available; it may be wrapped as well
    influencer_info = None
    if isinstance(raw_emotion, dict):
        inf = raw_emotion.get("influencer_styled") or raw_emotion.get("influencer")
        if isinstance(inf, dict) and inf.get("parsed") is not None:
            influencer_info = inf.get("parsed")
        else:
            influencer_info = inf

    # Defensive fixes: if influencer_info contains an error object, ignore it
    try:
        if isinstance(influencer_info, dict) and influencer_info.get('error'):
            influencer_info = None
    except Exception:
        influencer_info = influencer_info

    try:
        # if not found, check wrapped/raw payloads (various shapes)
        if isinstance(emotion_wrapped, dict):
            # common nested locations
            candidates = [emotion_wrapped.get('raw_model_output'), emotion_wrapped.get('raw'), emotion_wrapped.get('parsed')]
            for cand in candidates:
                try:
                    if isinstance(cand, dict):
                        mo = cand.get('model_output') or cand
                except Exception:
                    continue

        # also check orch_resp top-level dict forms if available
        if isinstance(orch_resp, dict):
            try:
                er = orch_resp.get('emotion') or {}
                if isinstance(er, dict):
                    rm = er.get('raw_model_output') or er.get('raw') or er.get('parsed')
                    if isinstance(rm, dict):
                        mo = rm.get('model_output') or rm
            except Exception:
                pass

    except Exception:
        pass

    # If influencer_info is missing or invalid, try a safe fallback: generate a short
    # influencer-styled message using the available color/emotion outputs via OpenAI.
    # This ensures the response follows the desired chain: color -> emotion -> influencer.
    try:
        if not influencer_info:
            # build a compact prompt summarizing color + emotion outputs
            try:
                color_summary = ''
                if isinstance(color_res, dict):
                    hints = color_res.get('detected_color_hints') or color_res.get('detected_color_hints') or {}
                    if isinstance(hints, dict):
                        color_summary = hints.get('result_name') or hints.get('reason') or ''
                emotion_summary = ''
                if isinstance(emotion_res, dict):
                    emotion_summary = emotion_res.get('description') or emotion_res.get('primary_tone') or ''

                system_msg = (
                    "ë‹¹ì‹ ì€ í•œêµ­ì–´ë¡œ ìì—°ìŠ¤ëŸ½ê³  ì¹œê·¼í•œ ì¸í”Œë£¨ì–¸ì„œ ë§íˆ¬ë¥¼ ëª¨ë°©í•˜ëŠ” ì¹´í”¼ë¼ì´í„°ì…ë‹ˆë‹¤. "
                    "ì‚¬ìš©ìì—ê²Œ ë°”ë¡œ ë³´ì—¬ì¤„ ìˆ˜ ìˆëŠ” 2~3ë¬¸ì¥ ë¶„ëŸ‰ì˜ ì‘ë‹µì„ ìƒì„±í•˜ì„¸ìš”."
                )
                user_msg = (
                    f"ì‚¬ìš©ì ìƒí™©: {emotion_summary}\ní¼ìŠ¤ë„ ì»¬ëŸ¬ íŒíŠ¸: {color_summary}\n"
                    "ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¹œê·¼í•˜ê³  ìƒë‹´ìë‹¤ìš´ ë§íˆ¬ë¡œ ê°„ë‹¨í•œ ì‘ë‹µì„ ë§Œë“¤ì–´ì£¼ì„¸ìš”."
                )

                resp = client.chat.completions.create(
                    model=get_model_to_use(),
                    messages=[{"role": "system", "content": system_msg}, {"role": "user", "content": user_msg}],
                    max_tokens=200,
                    temperature=0.7,
                )
                styled = ''
                try:
                    styled = resp.choices[0].message.content.strip()
                except Exception:
                    styled = str(resp)[:500]
                if styled:
                    influencer_info = {"styled_text": styled, "generated_by": "fallback_openai"}
            except Exception as e:
                # if OpenAI fallback fails, keep influencer_info as None
                print(f"[analyze] influencer fallback generation failed: {e}")
    except Exception:
        pass

    # Compose the data payload to store and return (keep structure compatible with frontend)
    data = {}
    # primary/sub tones: prefer personal-color hints from color service, fallback to emotion
    primary = None
    sub = None
    if isinstance(color_res, dict):
        detected = color_res.get("detected_color_hints") or {}
        primary = detected.get("primary_tone")
        sub = detected.get("sub_tone")
    if not primary and isinstance(emotion_res, dict):
        primary = emotion_res.get("primary_tone")
    if not sub and isinstance(emotion_res, dict):
        sub = emotion_res.get("sub_tone")

    # Normalize arbitrary model/free-text tones to canonical values
    try:
        norm_primary, norm_sub = normalize_personal_color(primary, sub)
        primary = norm_primary
        sub = norm_sub
    except Exception:
        # if normalization fails for any reason, fall back to raw values
        pass

    data["primary_tone"] = primary or ""
    data["sub_tone"] = sub or ""

    # description: influencer styled text (string) > influencer fields > emotion.description > color.description
    desc = None
    try:
        # influencer_info might be a string (already-styled text) or a dict with fields
        if isinstance(influencer_info, str) and influencer_info.strip():
            desc = influencer_info
        elif isinstance(influencer_info, dict):
            # prefer explicit styled_text, then description, then model_output.description
            desc = influencer_info.get('styled_text') or influencer_info.get('description')
            if not desc:
                mo = influencer_info.get('model_output') or influencer_info.get('raw') or None
                if isinstance(mo, dict):
                    desc = mo.get('description') or mo.get('styled_text')
    except Exception:
        desc = None

    if not desc:
        desc = (emotion_res.get("description") if isinstance(emotion_res, dict) else None) or (color_res.get("description") if isinstance(color_res, dict) else None)
    data["description"] = desc or "ì•ˆë…•í•˜ì„¸ìš”! í¼ìŠ¤ë„ì»¬ëŸ¬ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì–´ë–¤ ë¶€ë¶„ì´ ê³ ë¯¼ì´ì‹ ê°€ìš”?"

    # recommendations: merge lists from emotion, color, and influencer (if any)
    recs = []
    if isinstance(emotion_res, dict):
        recs.extend(emotion_res.get("recommendations", []) or [])
    if isinstance(color_res, dict):
        recs.extend(color_res.get("recommendations", []) or [])
    # influencer may include explicit recommendations
    if influencer_info and isinstance(influencer_info, dict):
        if influencer_info.get("recommendations"):
            recs.extend(influencer_info.get("recommendations"))

    # flatten and dedupe
    flat = []
    for item in recs:
        if isinstance(item, list):
            for subit in item:
                if isinstance(subit, str) and subit not in flat:
                    flat.append(subit)
        elif isinstance(item, str):
            if item not in flat:
                flat.append(item)
    if not flat:
        flat = ["ë” ìì„¸í•œ ì •ë³´ë¥¼ ìœ„í•´ í”¼ë¶€í†¤ì´ë‚˜ ì„ í˜¸ ìƒ‰ì„ ì•Œë ¤ì£¼ì„¸ìš”."]
    data["recommendations"] = flat

    # attach influencer metadata for frontend
    if influencer_info:
        data["influencer"] = influencer_info

    # Resolve emotion tag (orchestrator -> api_emotion -> local detector)
    # Fast pre-check: if the user's message or recent convo contains strong anger/fear cues,
    # short-circuit and use that label before calling external services.
    convo_text = "\n".join([c.get("text", "") for c in convo_list]) if convo_list else ""
    precheck_label = _precheck_strong_anger_fear(request.question, convo_text)
    if precheck_label:
        user_emotion = precheck_label
    else:
        # If this analyze call appears to be a welcome / image-upload prompt,
        # or the orchestrator explicitly marked it as a welcome, skip emotion
        # resolution and default to neutral to avoid UX confusion.
        try:
            # detect welcome flag coming from orchestrator (various shapes)
            is_welcome_meta = False
            try:
                meta = None
                if isinstance(orch_resp, dict):
                    meta = orch_resp.get('_meta') or orch_resp.get('meta')
                elif hasattr(orch_resp, 'dict'):
                    try:
                        orch_dict = orch_resp.dict()
                        meta = orch_dict.get('_meta') or orch_dict.get('meta')
                    except Exception:
                        meta = getattr(orch_resp, 'meta', None)
                if isinstance(meta, dict) and meta.get('is_welcome'):
                    is_welcome_meta = True
            except Exception:
                is_welcome_meta = False

            qtxt = request.question or ''
            if is_welcome_meta or (isinstance(qtxt, str) and re.search(r"ì´ë¯¸ì§€|ì—…ë¡œë“œ|í™˜ì˜|í™˜ì˜í•©ë‹ˆë‹¤|í™˜ì˜í•´", qtxt)):
                print('[analyze] welcome-like detected (meta or question); forcing emotion=neutral')
                user_emotion = 'neutral'
            else:
                user_emotion = await _resolve_emotion_tag(emotion_res, convo_list, request.question)
        except Exception:
            user_emotion = await _resolve_emotion_tag(emotion_res, convo_list, request.question)
    # canonicalize and attach emotion + lottie filename for frontend
    user_emotion = to_canonical(user_emotion)
    data["emotion"] = user_emotion
    # provide the frontend with the exact lottie filename it should load
    data["emotion_lottie"] = lottie_filename(user_emotion)
    # Store a human-readable message in the `text` field so the frontend
    # doesn't render a raw JSON blob. Prefer the `description` (influencer-styled
    # text) when available; fall back to the full JSON payload string.
    human_text = data.get("description") or json.dumps(data, ensure_ascii=False)
    # Store both human-friendly text and the structured payload as `raw`.
    ai_msg = models.ChatMessage(
        history_id=chat_history.id,
        role="ai",
        text=human_text,
        raw=json.dumps({
            "primary_tone": data.get("primary_tone"),
            "sub_tone": data.get("sub_tone"),
            "description": data.get("description"),
            "recommendations": data.get("recommendations"),
            "influencer": data.get("influencer"),
            "emotion": data.get("emotion"),
            "emotion_lottie": data.get("emotion_lottie"),
        }, ensure_ascii=False),
    )
    db.add(ai_msg)
    db.commit()
    db.refresh(ai_msg)

    # AI ë‹µë³€ ì €ì¥ í›„, AI í”¼ë“œë°± ìë™ í‰ê°€ ì‹¤í–‰ (ì±„íŒ… ì¢…ë£Œ ì „ì—ë„ í‰ê°€ ê°€ëŠ¥í•˜ë„ë¡ ì˜ˆì™¸ ë¬´ì‹œ)
    try:
        generate_ai_feedbacks(history_id=chat_history.id, current_user=current_user, db=db)
    except Exception as e:
        # ì˜ˆ: ì±„íŒ… ì¢…ë£Œ ì „ì—ëŠ” í‰ê°€ ë¶ˆê°€ ë“±ì˜ ì˜ˆì™¸ ë°œìƒ ê°€ëŠ¥, ë¬´ì‹œí•˜ê³  ì§„í–‰
        pass
    msgs = db.query(models.ChatMessage).filter_by(history_id=chat_history.id).order_by(models.ChatMessage.id.asc()).all()
    items = []
    qid = 1
    i = 0
    # Robust pairing: for each user message, find the next AI message (if any)
    while i < len(msgs):
        try:
            if msgs[i].role == 'user':
                # find next ai message
                j = i + 1
                while j < len(msgs) and msgs[j].role != 'ai':
                    j += 1
                if j < len(msgs) and msgs[j].role == 'ai':
                    ai_msg = msgs[j]
                    raw_blob = getattr(ai_msg, 'raw', None) or (ai_msg.text or "")
                    d = None
                    try:
                        if isinstance(raw_blob, str):
                            d = json.loads(raw_blob)
                        elif isinstance(raw_blob, dict):
                            d = raw_blob
                        else:
                            d = {"description": str(raw_blob)}
                    except Exception:
                        try:
                            text_blob = ai_msg.text or ""
                            d = json.loads(text_blob)
                        except Exception:
                            d = {"description": ai_msg.text or ""}

                    # normalize nested description/json
                    if isinstance(d.get("description"), str):
                        desc_text = d.get("description", "").strip()
                        if desc_text.startswith("{") or desc_text.startswith("["):
                            try:
                                parsed_desc = json.loads(desc_text)
                                if isinstance(parsed_desc, dict):
                                    for k, v in parsed_desc.items():
                                        if k not in d or k == 'description':
                                            d[k] = v
                            except Exception:
                                pass

                    recommendations = d.get("recommendations", [])
                    if isinstance(recommendations, dict):
                        recommendations = list(recommendations.values())
                    elif isinstance(recommendations, list):
                        flattened_recommendations = []
                        for item in recommendations:
                            if isinstance(item, list):
                                flattened_recommendations.extend(item)
                            elif isinstance(item, str):
                                flattened_recommendations.append(item)
                        recommendations = flattened_recommendations
                    else:
                        recommendations = []
                    d["recommendations"] = recommendations
                    d.setdefault('primary_tone', '')
                    d.setdefault('sub_tone', '')
                    d.setdefault('emotion', d.get('emotion', 'neutral') or 'neutral')
                    d.setdefault('description', d.get('description') or '')

                    items.append(ChatItemModel(
                        question_id=qid,
                        question=msgs[i].text,
                        answer=d.get("description",""),
                        chat_res=ChatResModel.model_validate(d),
                        emotion=d.get("emotion", "neutral")
                    ))
                    qid += 1
                    # advance i to after this ai message
                    i = j + 1
                    continue
            i += 1
        except Exception:
            i += 1
    return {"history_id": chat_history.id, "items": items}


@router.post("/start")
def start_chat_session(
    payload: dict | None = Body(None),
    current_user: models.User = Depends(get_current_user),
    db: Session = Depends(get_db),
):
    """
    ëª…ì‹œì ìœ¼ë¡œ ìƒˆ ì±„íŒ… ì„¸ì…˜ì„ ìƒì„±í•˜ê³  history_idë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    í”„ë¡ íŠ¸ì—”ë“œê°€ í˜ì´ì§€ ì§„ì… ì‹œ ì´ ì—”ë“œí¬ì¸íŠ¸ë¥¼ í˜¸ì¶œí•˜ì—¬
    ê¸°ì¡´ ì—´ë¦° ì„¸ì…˜ê³¼ ê´€ê³„ì—†ì´ í•­ìƒ ìƒˆë¡œìš´ ì„¸ì…˜ì„ ì‹œì‘í•˜ë„ë¡ í•©ë‹ˆë‹¤.
    """
    # DB-level concurrency handling:
    # Acquire a FOR UPDATE lock on the user row, then check for an open ChatHistory.
    # This prevents two concurrent requests from both observing "no open session" and
    # creating duplicate open sessions. Locking the user row is lightweight and
    # avoids requiring DB schema changes (partial unique indexes) here.
    try:
        # optional influencer_name from request body
        influencer_name = None
        try:
            if payload and isinstance(payload, dict):
                influencer_name = payload.get('influencer_name') or payload.get('influencer')
        except Exception:
            influencer_name = None

        # Lock the user row for this transaction
        db.query(models.User).filter(models.User.id == current_user.id).with_for_update().first()

        # Now check again for an existing open session while holding the lock
        # If an influencer_name was requested, prefer reusing an open session for that influencer
        existing = None
        if influencer_name:
            try:
                existing = db.query(models.ChatHistory).filter(
                    models.ChatHistory.user_id == current_user.id,
                    models.ChatHistory.ended_at == None,
                    models.ChatHistory.influencer_name == influencer_name,
                ).order_by(models.ChatHistory.created_at.desc()).first()
            except Exception:
                existing = None

        # fallback: any existing open session
        if not existing:
            existing = db.query(models.ChatHistory).filter(
                models.ChatHistory.user_id == current_user.id,
                models.ChatHistory.ended_at == None,
            ).order_by(models.ChatHistory.created_at.desc()).first()

        if existing:
            user_turns = db.query(models.ChatMessage).filter_by(history_id=existing.id, role='user').count()
            print(f"ğŸ” ê¸°ì¡´ ì—´ë¦° ì„¸ì…˜ ì¬ì‚¬ìš©: user_id={current_user.id}, history_id={existing.id}, user_turns={user_turns}")
            return {"history_id": existing.id, "reused": True, "user_turns": user_turns}

        # No existing open session found while holding the lock: create one
        chat_history = models.ChatHistory(user_id=current_user.id)
        # persist both influencer id and name when available
        try:
            if influencer_name:
                # if influencer_name is actually an id (slug), store in influencer_id
                if isinstance(influencer_name, str) and '_' in influencer_name:
                    chat_history.influencer_id = influencer_name
                else:
                    chat_history.influencer_name = influencer_name
        except Exception:
            pass
        db.add(chat_history)
        db.commit()
        db.refresh(chat_history)
        print(f"â• ìƒˆ ì±„íŒ… ì„¸ì…˜ ìƒì„±: user_id={current_user.id}, history_id={chat_history.id}")
        return {"history_id": chat_history.id, "reused": False, "user_turns": 0}
    except Exception as e:
        # Roll back on error and return a 500 so clients can retry safely
        print(f"âŒ /start ì˜¤ë¥˜ ë°œìƒ: {e}")
        try:
            db.rollback()
        except:
            pass
        raise HTTPException(status_code=500, detail="ì±„íŒ… ì„¸ì…˜ ìƒì„± ì¤‘ DB ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤")




@router.get('/history/influencers')
def get_influencer_histories(current_user: models.User = Depends(get_current_user), db: Session = Depends(get_db)):
    """Return a list of influencer groups for the current user with simple summaries.

    Each item contains: `influencer_id`, `influencer_name`, `total_sessions`, `total_messages`, `last_activity`.
    """
    try:
        user_id = getattr(current_user, 'id', None)
        if not user_id:
            raise HTTPException(status_code=401, detail="ë¡œê·¸ì¸ í•„ìš”")

        histories = db.query(models.ChatHistory).filter_by(user_id=user_id).order_by(models.ChatHistory.created_at.desc()).all()

        groups: dict = {}
        for h in histories:
            key = h.influencer_id or (h.influencer_name or 'unknown')
            name = h.influencer_name or h.influencer_id or 'unknown'
            if key not in groups:
                groups[key] = {
                    'influencer_id': key,
                    'influencer_name': name,
                    'histories': [],
                    'total_messages': 0,
                    'last_activity': h.created_at,
                }
            groups[key]['histories'].append(h.id)
            # count messages for this history
            try:
                cnt = db.query(models.ChatMessage).filter_by(history_id=h.id).count()
            except Exception:
                cnt = 0
            groups[key]['total_messages'] += cnt
            if h.created_at and (not groups[key]['last_activity'] or h.created_at > groups[key]['last_activity']):
                groups[key]['last_activity'] = h.created_at

        # Retrieve influencer profiles (prefer influencer service) to merge metadata
        profiles = None
        try:
            if influencer_service and hasattr(influencer_service, 'influencer_profiles'):
                res = influencer_service.influencer_profiles()
                if isinstance(res, list):
                    outp = []
                    for it in res:
                        try:
                            if hasattr(it, 'dict'):
                                outp.append(it.dict())
                            else:
                                outp.append(it)
                        except Exception:
                            outp.append(it)
                    profiles = outp
                else:
                    profiles = res
        except Exception:
            profiles = None

        # fallback safe list if service not available
        if not profiles:
            profiles = [
                {'id': 'won_jun', 'name': 'ì›ì¤€', 'short_description': 'ì¹œê·¼í•˜ë©´ì„œë„ ì†”ì§í•œ ë¦¬ë·°', 'example_sentences': ['ì•ˆë…•í•˜ì„¸ìš” ê·€ìš¤ì´ë‹˜! ì›ì¤€ì…ë‹ˆë‹¤!']},
                {'id': 'se_hyun', 'name': 'ì„¸í˜„', 'short_description': 'ìì—°ìŠ¤ëŸ¬ìš´ ë°ì¼ë¦¬ ë©”ì´í¬ì—… ì „ë¬¸', 'example_sentences': ['ì•ˆë…•í•˜ì„¸ìš” í¬ë“œë˜ê³¤ë‹˜! ì„¸í˜„ì´ì˜ˆìš”!']},
                {'id': 'jong_min', 'name': 'ì¢…ë¯¼', 'short_description': 'ê°€ì„±ë¹„ ì¤‘ì‹¬ì˜ ì‹¤ìš©ì  ë¦¬ë·°', 'example_sentences': ['ì•ˆë…•í•˜ì„¸ìš” íŠ¸ë£¨ë“œë˜ê³¤ë‹˜! ì¢…ë¯¼ì…ë‹ˆë‹¤!']},
                {'id': 'hye_kyung', 'name': 'í˜œê²½', 'short_description': 'ì¢…í•© ë·°í‹° ê°€ì´ë“œ', 'example_sentences': ['ì•ˆë…•í•˜ì„¸ìš” ë·°í‹°íŒ¨ë°€ë¦¬ë‹˜! í˜œê²½ì…ë‹ˆë‹¤!']},
            ]

        # Normalize profiles into a lookup by id and by name
        profile_map_by_id = {}
        profile_map_by_name = {}
        for p in profiles:
            try:
                if isinstance(p, dict):
                    pid = p.get('id') or p.get('influencer_id') or None
                    name = p.get('name') or p.get('short_name') or None
                    if pid:
                        profile_map_by_id[str(pid)] = p
                    if name:
                        profile_map_by_name[str(name).lower()] = p
            except Exception:
                continue

        # Ensure that every known profile appears in the groups map even if the user
        # has no chat histories with them. This lets the frontend depend on a
        # single endpoint for both the influencer list and per-influencer histories.
        def _slugify_name(n: str) -> str:
            try:
                s = str(n).strip().lower()
                s = s.replace(' ', '_')
                import re
                s = re.sub(r'[^a-z0-9_\-]', '', s)
                return s
            except Exception:
                return str(n)

        for p in profiles:
            try:
                if not isinstance(p, dict):
                    continue
                pid = p.get('id') or p.get('influencer_id')
                name = p.get('name') or p.get('short_name') or p.get('short_description')
                key = str(pid) if pid else _slugify_name(name or 'unknown')
                if key not in groups:
                    groups[key] = {
                        'influencer_id': key,
                        'influencer_name': name or key,
                        'histories': [],
                        'total_messages': 0,
                        'last_activity': None,
                    }
            except Exception:
                continue

        # Remove the generic 'unknown' group so the frontend receives only
        # meaningful influencer entries (profiles or named influencers).
        # This avoids showing an 'unknown' tile in the influencer list.
        filtered_groups = {k: v for k, v in groups.items() if str(k).lower() != 'unknown'}

        out = []
        for key, g in filtered_groups.items():
            recent_msg = None
            try:
                recent_msg = db.query(models.ChatMessage).join(models.ChatHistory).filter(models.ChatHistory.user_id==user_id, models.ChatHistory.id.in_(g['histories'])).order_by(models.ChatMessage.created_at.desc()).first()
            except Exception:
                recent_msg = None

            short = None
            if recent_msg:
                text = getattr(recent_msg, 'text', '') or ''
                short = text.replace('\n', ' ').strip()
                if len(short) > 120:
                    short = short[:117] + '...'

            # merge profile metadata if available
            profile_meta = None
            try:
                # prefer exact id match
                if g['influencer_id'] and profile_map_by_id.get(str(g['influencer_id'])):
                    profile_meta = profile_map_by_id.get(str(g['influencer_id']))
                else:
                    # try name match
                    nm = (g['influencer_name'] or '').lower()
                    if nm and profile_map_by_name.get(nm):
                        profile_meta = profile_map_by_name.get(nm)
            except Exception:
                profile_meta = None

            # prefer message-level timestamp for last_activity when available
            last_activity_val = g.get('last_activity')
            try:
                if recent_msg and getattr(recent_msg, 'created_at', None):
                    # if recent_msg is newer than the history-level last_activity, prefer it
                    if not last_activity_val or getattr(recent_msg, 'created_at') > last_activity_val:
                        last_activity_val = getattr(recent_msg, 'created_at')
            except Exception:
                pass

            item = {
                'influencer_id': g['influencer_id'],
                'influencer_name': g['influencer_name'],
                'total_sessions': len(g['histories']),
                'total_messages': g['total_messages'],
                'last_activity': last_activity_val,
            }

            # Aggregate numeric ratings for this influencer across its histories
            try:
                avg_cnt = db.query(
                    func.avg(models.UserFeedback.rating),
                    func.count(models.UserFeedback.id)
                ).filter(
                    models.UserFeedback.history_id.in_(g['histories']),
                    models.UserFeedback.rating != None
                ).one()
                avg_val, cnt_val = avg_cnt[0], avg_cnt[1]
                item['average_rating'] = float(avg_val) if avg_val is not None else None
                item['rating_count'] = int(cnt_val or 0)
            except Exception:
                item['average_rating'] = None
                item['rating_count'] = 0

            # Ensure profile object exists and attach short_description inside profile
            try:
                if profile_meta and isinstance(profile_meta, dict):
                    full_profile = dict(profile_meta)
                else:
                    # create a minimal profile object so frontend always finds `profile.short_description`
                    full_profile = {'id': g['influencer_id'] or g['influencer_name'], 'name': g['influencer_name']}

                # normalize aliases
                if not full_profile.get('id'):
                    full_profile['id'] = full_profile.get('influencer_id') or full_profile.get('name')

                # prefer recent message; otherwise keep existing profile short_description or description
                existing_short = full_profile.get('short_description') or full_profile.get('short_name') or full_profile.get('description') or ''
                full_profile['short_description'] = short or existing_short or ''

                item['profile'] = full_profile

            except Exception:
                # fallback: attach a minimal profile with empty short_description
                item['profile'] = {'id': g['influencer_id'] or g['influencer_name'], 'name': g['influencer_name'], 'short_description': short or ''}

            out.append(item)

        # sort by last_activity desc
        out.sort(key=lambda x: x.get('last_activity') or datetime.min, reverse=True)
        return out
    except HTTPException:
        raise
    except Exception as e:
        print(f"[get_influencer_histories] error: {e}")
        raise HTTPException(status_code=500, detail="ì¸í”Œë£¨ì–¸ì„œë³„ íˆìŠ¤í† ë¦¬ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤")


@router.get('/history/{history_id}')
def get_chat_history(history_id: int, current_user: models.User = Depends(get_current_user), db: Session = Depends(get_db)):
    """Return existing chat history items for the current user.

    This endpoint is safe to call after `start` returns a history_id (including reused sessions)
    and will return the same `items` structure as `/analyze` produces so the frontend can
    rehydrate the chat UI without sending a new user message.
    """
    try:
        history = db.query(models.ChatHistory).filter_by(id=history_id, user_id=current_user.id).first()
        if not history:
            raise HTTPException(status_code=404, detail="í•´ë‹¹ history_id ì„¸ì…˜ ì—†ìŒ")

        msgs = db.query(models.ChatMessage).filter_by(history_id=history.id).order_by(models.ChatMessage.id.asc()).all()
        items = []
        qid = 1
        # Robust pairing: for each user message, find the next AI/system/assistant message
        i = 0
        while i < len(msgs):
            try:
                if msgs[i].role == 'user':
                    j = i + 1
                    while j < len(msgs) and msgs[j].role not in ('ai', 'system', 'assistant'):
                        j += 1
                    if j < len(msgs):
                        ai_msg = msgs[j]
                        raw_blob = getattr(ai_msg, 'raw', None) or (ai_msg.text or "")
                        d = None
                        try:
                            if isinstance(raw_blob, str):
                                d = json.loads(raw_blob)
                            elif isinstance(raw_blob, dict):
                                d = raw_blob
                            else:
                                d = {"description": str(raw_blob)}
                        except Exception:
                            try:
                                text_blob = ai_msg.text or ""
                                d = json.loads(text_blob)
                            except Exception:
                                d = {"description": ai_msg.text or ""}

                        # normalize recommendations field
                        recommendations = d.get('recommendations', [])
                        if isinstance(recommendations, dict):
                            recommendations = list(recommendations.values())
                        elif isinstance(recommendations, list):
                            flattened_recommendations = []
                            for item in recommendations:
                                if isinstance(item, list):
                                    flattened_recommendations.extend(item)
                                elif isinstance(item, str):
                                    flattened_recommendations.append(item)
                            recommendations = flattened_recommendations
                        else:
                            recommendations = []
                        d['recommendations'] = recommendations
                        d.setdefault('primary_tone', '')
                        d.setdefault('sub_tone', '')
                        d.setdefault('emotion', d.get('emotion', 'neutral') or 'neutral')
                        d.setdefault('description', d.get('description') or '')

                        # create ChatItemModel-like structure
                        item = {
                            'question_id': qid,
                            'question': msgs[i].text,
                            'answer': d.get('description', ''),
                            'chat_res': d,
                            # include timestamps so clients can render original message times
                            'question_created_at': (msgs[i].created_at.isoformat() if getattr(msgs[i], 'created_at', None) else None),
                            'created_at': (msgs[j].created_at.isoformat() if getattr(msgs[j], 'created_at', None) else None),
                        }
                        items.append(item)
                        qid += 1
                        i = j + 1
                        continue
                i += 1
            except Exception:
                i += 1

        return {"history_id": history.id, "items": items}
    except HTTPException:
        raise
    except Exception as e:
        print(f"[get_chat_history] error: {e}")
        raise HTTPException(status_code=500, detail="íˆìŠ¤í† ë¦¬ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤")


@router.get('/history/influencer/{influencer_id}')
def get_messages_for_influencer(influencer_id: str, current_user: models.User = Depends(get_current_user), db: Session = Depends(get_db)):
    """Return all messages for the given influencer across the user's chat histories.

    The response contains `history_id` and `items` (list of messages ordered by created_at asc).
    """
    try:
        user_id = getattr(current_user, 'id', None)
        if not user_id:
            raise HTTPException(status_code=401, detail="ë¡œê·¸ì¸ í•„ìš”")

        # find histories that match influencer_id (exact match on influencer_id OR name-like match)
        histories = db.query(models.ChatHistory).filter(models.ChatHistory.user_id==user_id).filter(
            (models.ChatHistory.influencer_id == influencer_id) | (models.ChatHistory.influencer_name.like(f"%{influencer_id}%"))
        ).order_by(models.ChatHistory.created_at.asc()).all()

        if not histories:
            return {'history_id': None, 'items': []}

        # collect messages across histories, preserving chronological order
        history_ids = [h.id for h in histories]
        msgs = db.query(models.ChatMessage).filter(models.ChatMessage.history_id.in_(history_ids)).order_by(models.ChatMessage.created_at.asc()).all()

        items = []
        for m in msgs:
            try:
                raw_val = getattr(m, 'raw', None)
                parsed = None
                # Try to obtain a parsed dict from raw (preferred)
                if raw_val:
                    if isinstance(raw_val, dict):
                        parsed = raw_val
                    elif isinstance(raw_val, str):
                        try:
                            parsed = json.loads(raw_val)
                        except Exception:
                            parsed = None
                # If we couldn't parse raw, try parsing the text (older records stored JSON in text)
                if parsed is None:
                    txt = getattr(m, 'text', '') or ''
                    if isinstance(txt, dict):
                        parsed = txt
                    elif isinstance(txt, str) and (txt.strip().startswith('{') or txt.strip().startswith('[')):
                        try:
                            parsed = json.loads(txt)
                        except Exception:
                            parsed = None

                # Normalize parsed into a dict-like structure for the frontend
                if not isinstance(parsed, dict):
                    # fallback: keep raw as-is inside a description
                    parsed = {'description': (getattr(m, 'text', '') or '')}

                # If the parsed payload contains nested JSON inside `description` or `styled_text`, try to unwrap
                if isinstance(parsed.get('description'), str):
                    desc_candidate = parsed.get('description').strip()
                    if desc_candidate.startswith('{') or desc_candidate.startswith('['):
                        try:
                            inner = json.loads(desc_candidate)
                            if isinstance(inner, dict):
                                # merge keys from parsed_desc into d without overwriting existing top-level fields
                                for k, v in inner.items():
                                    if k not in parsed or k == 'description':
                                        parsed[k] = v
                        except Exception:
                            pass

                # Prefer influencer.styled_text when available
                styled_text = None
                infl = parsed.get('influencer')
                if isinstance(infl, dict):
                    # some records embed another JSON string inside influencer.raw/raw.model_output
                    st = infl.get('styled_text') or infl.get('description') or None
                    if isinstance(st, str) and (st.strip().startswith('{') or st.strip().startswith('[')):
                        try:
                            stp = json.loads(st)
                            if isinstance(stp, dict) and stp.get('styled_text'):
                                styled_text = stp.get('styled_text')
                            else:
                                # If the nested value is actually a dict describing styled_text, try common keys
                                styled_text = stp.get('styled_text') or stp.get('description') or None
                        except Exception:
                            styled_text = st
                    else:
                        styled_text = st

                # fallback to top-level styled_text or description
                if not styled_text:
                    top_st = parsed.get('styled_text') or parsed.get('description') or None
                    if isinstance(top_st, str) and (top_st.strip().startswith('{') or top_st.strip().startswith('[')):
                        try:
                            inner_top = json.loads(top_st)
                            if isinstance(inner_top, dict):
                                styled_text = inner_top.get('styled_text') or inner_top.get('description') or None
                            else:
                                styled_text = str(top_st)
                        except Exception:
                            styled_text = str(top_st)
                    else:
                        styled_text = top_st

                # final_clean_text: ensure it's a simple string
                final_text = styled_text if isinstance(styled_text, str) and styled_text.strip() else (parsed.get('description') or (getattr(m, 'text', '') or ''))

                items.append({
                    'history_id': m.history_id,
                    'role': m.role,
                    'text': final_text,
                    'raw': parsed,
                    'created_at': m.created_at.isoformat() if getattr(m, 'created_at', None) else None,
                })
            except Exception:
                # fallback to original minimal representation on unexpected errors
                items.append({
                    'history_id': m.history_id,
                    'role': m.role,
                    'text': getattr(m, 'text', '') or '',
                    'raw': getattr(m, 'raw', None),
                    'created_at': m.created_at.isoformat() if getattr(m, 'created_at', None) else None,
                })

        return {'history_ids': history_ids, 'items': items}
    except Exception as e:
        print(f"[get_messages_for_influencer] error: {e}")
        raise HTTPException(status_code=500, detail="ì¸í”Œë£¨ì–¸ì„œë³„ ë©”ì‹œì§€ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤")
    

@router.post("/end/{history_id}")
async def end_chat_session(
    history_id: int,
    current_user: models.User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    chat = db.query(models.ChatHistory).filter_by(id=history_id, user_id=current_user.id).first()
    if not chat:
        raise HTTPException(status_code=404, detail="ëŒ€í™” ì„¸ì…˜ ì—†ìŒ")
    if chat.ended_at:
        return {"message": "ì´ë¯¸ ì¢…ë£Œë¨", "ended_at": chat.ended_at}
    
    # ëŒ€í™” ì¢…ë£Œ ì‹œê°„ ì„¤ì •
    chat.ended_at = datetime.now(timezone.utc)
    db.commit()
    
    # ì±—ë´‡ ëŒ€í™” ë¶„ì„ ê²°ê³¼ë¥¼ SurveyResultë¡œ ì €ì¥
    try:
        survey_result = await save_chatbot_analysis_result(
            user_id=current_user.id,
            chat_history_id=history_id,
            db=db
        )
        
        if survey_result:
            return {
                "message": "ëŒ€í™” ì¢…ë£Œ ë° ë¶„ì„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ", 
                "ended_at": chat.ended_at,
                "survey_result_id": survey_result.id,
                "personal_color_type": survey_result.result_tone
            }
        else:
            return {
                "message": "ëŒ€í™” ì¢…ë£Œë¨ (ë¶„ì„ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨)", 
                "ended_at": chat.ended_at
            }
            
    except Exception as e:
        print(f"âŒ ë¶„ì„ ê²°ê³¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
        return {
            "message": "ëŒ€í™” ì¢…ë£Œë¨ (ë¶„ì„ ê²°ê³¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ)", 
            "ended_at": chat.ended_at
        }


@router.post("/report/request")
async def request_personal_color_report(
    request_data: dict,
    current_user: models.User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """
    ğŸ”¥ ê¸°ì¡´ í¼ìŠ¤ë„ ì»¬ëŸ¬ ì§„ë‹¨ ë³´ê³ ì„œ ìƒì„± ìš”ì²­ ğŸ”¥
    
    âš ï¸ ì¤‘ìš”: ì´ APIëŠ” ìƒˆë¡œìš´ ì§„ë‹¨ ê¸°ë¡ì„ ìƒì„±í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤!
    - ê¸°ì¡´ ì§„ë‹¨ ê²°ê³¼(SurveyResult)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë¦¬í¬íŠ¸ë§Œ ìƒì„±
    - ì§„ë‹¨ ê¸°ë¡(ë§ˆì´í˜ì´ì§€)ì— ìƒˆë¡œìš´ í•­ëª©ì´ ì¶”ê°€ë˜ì§€ ì•ŠìŒ
    - ë‹¨ìˆœíˆ ê¸°ì¡´ ë°ì´í„°ë¥¼ ì‹œê°í™”/í¬ë§·íŒ…í•˜ì—¬ ë¦¬í¬íŠ¸ë¡œ ì œê³µ
    
    ìƒˆë¡œìš´ ì§„ë‹¨ ê¸°ë¡ì€ ì˜¤ì§ ëŒ€í™”í˜• ë¶„ì„ì„ í†µí•´ì„œë§Œ ìƒì„±ë©ë‹ˆë‹¤.
    """
    survey_result_id = request_data.get("history_id")  # ì‹¤ì œë¡œëŠ” survey_result_id
    
    if not survey_result_id:
        raise HTTPException(status_code=400, detail="ì§„ë‹¨ ê²°ê³¼ IDê°€ í•„ìš”í•©ë‹ˆë‹¤")
    
    # ì‚¬ìš©ìì˜ ê¸°ì¡´ ì§„ë‹¨ ê²°ê³¼ ì¡°íšŒ (ì½ê¸° ì „ìš©)
    survey_result = db.query(models.SurveyResult).filter_by(
        id=survey_result_id, 
        user_id=current_user.id, 
        is_active=True
    ).first()
    
    if not survey_result:
        raise HTTPException(status_code=404, detail="ì§„ë‹¨ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
    
    print(f"ğŸ“Š ê¸°ì¡´ ì§„ë‹¨ ê²°ê³¼ ê¸°ë°˜ ë¦¬í¬íŠ¸ ìƒì„±: survey_result_id={survey_result_id}")
    print(f"   - ê²°ê³¼ íƒ€ì…: {survey_result.result_tone}")
    print(f"   - ìƒì„±ì¼: {survey_result.created_at}")
    print(f"   â— ìƒˆë¡œìš´ ì§„ë‹¨ ê¸°ë¡ì„ ìƒì„±í•˜ì§€ ì•ŠìŒ (ë¦¬í¬íŠ¸ë§Œ ìƒì„±)")
    
    try:
        from utils.report_generator import PersonalColorReportGenerator
        
        # ë¦¬í¬íŠ¸ ìƒì„±ê¸° ì´ˆê¸°í™”
        report_generator = PersonalColorReportGenerator()
        
        # ê¸°ì¡´ ì§„ë‹¨ ê²°ê³¼ë¥¼ ë¦¬í¬íŠ¸ ë°ì´í„°ë¡œ ë³€í™˜ (ì½ê¸° ì „ìš©)
        survey_data = {
            "result_tone": survey_result.result_tone,
            "result_name": survey_result.result_name,
            "confidence": survey_result.confidence,
            "detailed_analysis": survey_result.detailed_analysis,
            "color_palette": survey_result.color_palette,
            "style_keywords": survey_result.style_keywords,
            "makeup_tips": survey_result.makeup_tips
        }
        
        # ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¡°íšŒ (ë¦¬í¬íŠ¸ì— í¬í•¨í•  ëŒ€í™” ë‚´ìš©, ì½ê¸° ì „ìš©)
        chat_history = []
        if hasattr(survey_result, 'chat_history_id') and survey_result.chat_history_id:
            messages = db.query(models.ChatMessage).filter_by(
                history_id=survey_result.chat_history_id
            ).order_by(models.ChatMessage.created_at.asc()).all()
            
            chat_history = [
                {
                    "role": msg.role,
                    "text": msg.text,
                    "created_at": msg.created_at.isoformat()
                }
                for msg in messages
            ]
        
        # ë¦¬í¬íŠ¸ ë°ì´í„° ìƒì„± (ê¸°ì¡´ ë°ì´í„° ì‹œê°í™”ë§Œ, DB ë³€ê²½ ì—†ìŒ)
        report_data = report_generator.generate_report_data(survey_data, chat_history)
        
        # âš ï¸ ì¤‘ìš”: ì—¬ê¸°ì„œ db.add(), db.commit() ë“±ì˜ DB ë³€ê²½ ì‘ì—… ì ˆëŒ€ ê¸ˆì§€!
        print(f"âœ… ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ (DB ë³€ê²½ ì—†ìŒ)")
        
        return {
            "status": "success",
            "message": f"{survey_result.result_name or survey_result.result_tone.upper()} íƒ€ì… ë¶„ì„ ë¦¬í¬íŠ¸ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤",
            "survey_result_id": survey_result_id,
            "report_data": report_data,
            "note": "ê¸°ì¡´ ì§„ë‹¨ ë°ì´í„° ê¸°ë°˜ ë¦¬í¬íŠ¸ ìƒì„± (ìƒˆë¡œìš´ ì§„ë‹¨ ê¸°ë¡ ì¶”ê°€ ì—†ìŒ)"
        }
        
    except Exception as e:
        print(f"âŒ ë¦¬í¬íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"ë¦¬í¬íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

@router.get("/report/{survey_result_id}")
async def get_personal_color_report(
    survey_result_id: int,
    current_user: models.User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """
    ìƒì„±ëœ í¼ìŠ¤ë„ ì»¬ëŸ¬ ì§„ë‹¨ ë³´ê³ ì„œ ì¡°íšŒ
    """
    survey_result = db.query(models.SurveyResult).filter_by(
        id=survey_result_id, 
        user_id=current_user.id, 
        is_active=True
    ).first()
    
    if not survey_result:
        raise HTTPException(status_code=404, detail="ì§„ë‹¨ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
    
    try:
        from utils.report_generator import PersonalColorReportGenerator
        
        report_generator = PersonalColorReportGenerator()
        
        # ì§„ë‹¨ ê²°ê³¼ ë°ì´í„° ì¤€ë¹„
        survey_data = {
            "result_tone": survey_result.result_tone,
            "result_name": survey_result.result_name,
            "confidence": survey_result.confidence,
            "detailed_analysis": survey_result.detailed_analysis,
            "color_palette": survey_result.color_palette,
            "style_keywords": survey_result.style_keywords,
            "makeup_tips": survey_result.makeup_tips
        }
        
        # ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¡°íšŒ
        chat_history = []
        
        # ë¦¬í¬íŠ¸ ë°ì´í„° ìƒì„±
        report_data = report_generator.generate_report_data(survey_data, chat_history)
        
        # HTML ë¦¬í¬íŠ¸ë„ ìƒì„±
        html_report = report_generator.generate_html_report(report_data)
        
        return {
            "message": "ë¦¬í¬íŠ¸ ì¡°íšŒ ì„±ê³µ",
            "report_data": report_data,
            "html_report": html_report,
            "download_available": True
        }
        
    except Exception as e:
        print(f"âŒ ë¦¬í¬íŠ¸ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=f"ë¦¬í¬íŠ¸ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
