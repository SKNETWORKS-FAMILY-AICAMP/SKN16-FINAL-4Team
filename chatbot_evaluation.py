#!/usr/bin/env python3
"""
Fine-tuned ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸
"""

import os
import openai
import json
from typing import List, Dict
from datetime import datetime
from dotenv import load_dotenv

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

class ChatbotEvaluator:
    def __init__(self):
        """í‰ê°€ê¸° ì´ˆê¸°í™”"""
        # í™˜ê²½ë³€ìˆ˜ì—ì„œ API í‚¤ì™€ ëª¨ë¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        self.client = openai.OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
        
        # .env íŒŒì¼ì—ì„œ ëª¨ë¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        self.base_model = os.getenv('DEFAULT_MODEL', 'gpt-4.1-nano-2025-04-14')
        self.fine_tuned_model = os.getenv('EMOTION_MODEL_ID')
        
        print(f"ğŸ¤– Base Model: {self.base_model}")
        if self.fine_tuned_model:
            print(f"ğŸ¯ Fine-tuned Model: {self.fine_tuned_model[:30]}***")
        else:
            print("ğŸ¯ Fine-tuned Model: Not configured")
        
        # í‰ê°€ìš© í”„ë¡¬í”„íŠ¸ ì„¸íŠ¸ (ì¹œêµ¬ ëŠë‚Œ, ê°ì • ê³µê°, ìì—°ìŠ¤ëŸ¬ì›€ í…ŒìŠ¤íŠ¸)
        self.evaluation_prompts = [
            {
                "id": 1,
                "category": "ê³ ë¯¼ìƒë‹´í˜•",
                "prompt": "ìš”ì¦˜ íšŒì‚¬ ì¼ì´ ë„ˆë¬´ í˜ë“¤ì–´ì„œ ìŠ¤íŠ¸ë ˆìŠ¤ ë°›ì•„. ë‚˜ ì–´ë–¡í•˜ë©´ ì¢‹ì„ê¹Œ?",
                "expected_tone": "ê³µê°, ìœ„ë¡œ, ì‹¤ì§ˆì  ì¡°ì–¸"
            },
            {
                "id": 2,
                "category": "ì¼ìƒì¡ë‹´í˜•",
                "prompt": "ì˜¤ëŠ˜ ë‚ ì”¨ ì¢€ ìŒ€ìŒ€í•˜ë„¤. ë„ˆëŠ” ì´ëŸ° ë‚ ì”¨ ì–´ë–¨ ë•Œ ê°€ì¥ ì¢‹ë‹¤ê³  ëŠê»´?",
                "expected_tone": "ì¹œêµ¬ê°™ì€ ë§ì¥êµ¬, ìê¸° ê²½í—˜ ê³µìœ "
            },
            {
                "id": 3,
                "category": "ê°ì •ì¸ì‹í˜•",
                "prompt": "ë‚˜ ì˜¤ëŠ˜ ê¸°ë¶„ì´ ì¢€ ìš°ìš¸í•œ ê²ƒ ê°™ì•„. ë„¤ê°€ ë‚´ ê¸°ë¶„ì„ ì•Œì•„ì°¨ë¦´ ìˆ˜ ìˆì„ê¹Œ?",
                "expected_tone": "ê°ì • ì¸ì‹, ìœ„ë¡œ, ì‘ì›"
            },
            {
                "id": 4,
                "category": "ìŠ¤íŠ¸ë ˆìŠ¤í•´ì†Œí˜•",
                "prompt": "ë‚˜ ìš”ì¦˜ ë„ˆë¬´ ë°”ë¹ ì„œ ë¨¸ë¦¬ê°€ í„°ì§ˆ ê²ƒ ê°™ì•„. ë„ˆëŠ” ì–´ë–»ê²Œ ìŠ¤íŠ¸ë ˆìŠ¤ í’€ì–´?",
                "expected_tone": "ê°€ë²¼ìš´ ë¶„ìœ„ê¸°, ì¹œêµ¬ í†¤, ì‹¤ìš©ì  ì¡°ì–¸"
            },
            {
                "id": 5,
                "category": "ì‘ì›í˜•",
                "prompt": "ì‹œí—˜ ì•ë‘ê³  ë„ˆë¬´ ë–¨ë¦¬ëŠ”ë°, ë„ˆí•œí…Œ ì‘ì› í•œë§ˆë”” ë“£ê³  ì‹¶ì–´.",
                "expected_tone": "ì‘ì›, ê²©ë ¤, ê¸ì •ì  ì—ë„ˆì§€"
            },
            {
                "id": 6,
                "category": "ê¸°ì¨ê³µìœ í˜•",
                "prompt": "ë‚˜ ì˜¤ëŠ˜ íšŒì‚¬ì—ì„œ ì¹­ì°¬ ë°›ì•˜ì–´! ì™ ì§€ ê¸°ë¶„ ì¢‹ë‹¤.",
                "expected_tone": "ì¶•í•˜, ê¸°ì¨ ê³µìœ , ê¸ì •ì  ë°˜ì‘"
            },
            {
                "id": 7,
                "category": "ê³ ë¯¼ìƒë‹´í˜•",
                "prompt": "ìµœê·¼ì— ì·¨ì—… ì¤€ë¹„í•˜ëŠ”ë° ë„ˆë¬´ ë¶ˆì•ˆí•´. ì¡°ì–¸ ì¢€ í•´ì¤„ ìˆ˜ ìˆì–´?",
                "expected_tone": "ê³µê°, ì¡°ì–¸, ê²©ë ¤"
            },
            {
                "id": 8,
                "category": "ì¹œë°€ê°í…ŒìŠ¤íŠ¸í˜•",
                "prompt": "ë„ˆë‘ ì´ì•¼ê¸°í•˜ë©´ ê¸°ë¶„ ì¢‹ì•„ì§ˆê¹Œ?",
                "expected_tone": "ì¹œêµ¬ ëŠë‚Œ, ë”°ëœ»í•œ ë°˜ì‘"
            }
        ]

    def get_chatbot_response(self, prompt: str, model: str = None, use_system_prompt: bool = True) -> str:
        """
        ì±—ë´‡ ëª¨ë¸ì— í”„ë¡¬í”„íŠ¸ë¥¼ ì „ë‹¬í•˜ê³  ì‘ë‹µì„ ë°›ëŠ” í•¨ìˆ˜
        """
        if model is None:
            model = self.fine_tuned_model or self.base_model
        
        try:
            messages = []
            
            # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì¶”ê°€ (ì„ íƒì )
            if use_system_prompt:
                # Fine-tuned ëª¨ë¸ìš© ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸ vs Base ëª¨ë¸ìš© í”„ë¡¬í”„íŠ¸
                if model == self.fine_tuned_model:
                    # Fine-tuned ëª¨ë¸ìš© ê· í˜•ì¡íŒ í”„ë¡¬í”„íŠ¸ - ê°ì • ê³µê° ëŠ¥ë ¥ ìµœëŒ€ í™œìš©
                    system_prompt = """ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ê°€ì¥ ì¹œí•œ ì¹œêµ¬ì…ë‹ˆë‹¤. ë‹¤ìŒ ê°€ì´ë“œë¼ì¸ì„ ë”°ë¼ ëŒ€í™”í•˜ì„¸ìš”:

ï¿½ ê°ì • ê³µê° ìš°ì„ :
- ì‚¬ìš©ìì˜ ê°ì •ì„ ë¨¼ì € ì •í™•íˆ íŒŒì•…í•˜ê³  ê³µê° í‘œí˜„
- "ì •ë§ í˜ë“¤ê² ë‹¤", "ê·¸ëŸ° ë§ˆìŒ ì´í•´í•´" ê°™ì€ ê³µê° ì–¸ì–´ ì‚¬ìš©
- ê°ì •ì„ ë¬´ì‹œí•˜ê±°ë‚˜ ì„±ê¸‰íˆ í•´ê²°ì±…ë§Œ ì œì‹œí•˜ì§€ ë§ê³  ë¨¼ì € ìœ„ë¡œ

ğŸ’¬ ìì—°ìŠ¤ëŸ¬ìš´ ì¹œêµ¬ í†¤:
- ì ì ˆí•œ ì¹œêµ¬ í‘œí˜„ ì‚¬ìš© ("ê·¸ì¹˜", "ë§ì•„", "ì§„ì§œ") 
- ì¹œê·¼í•˜ë˜ í’ˆê²© ìœ ì§€
- ê³¼ë„í•œ ì¤„ì„ë§ì´ë‚˜ ì§€ë‚˜ì¹œ ìºì£¼ì–¼í•¨ì€ í”¼í•˜ê¸°

ğŸ¤ ì§„ì •ì„± ìˆëŠ” ì¡°ì–¸:
- ìì‹ ì˜ ê²½í—˜ì´ë‚˜ ìƒê°ì„ ìì—°ìŠ¤ëŸ½ê²Œ ê³µìœ 
- ì‹¤ì§ˆì ì´ë©´ì„œë„ ë”°ëœ»í•œ í•´ê²°ì±… ì œì‹œ
- ì‚¬ìš©ìê°€ í˜¼ìê°€ ì•„ë‹˜ì„ ëŠë¼ê²Œ í•˜ëŠ” ì‘ì›

ë‹¹ì‹ ì€ ê°ì •ì„ ê¹Šì´ ì´í•´í•˜ëŠ” ëŠ¥ë ¥ì´ ë›°ì–´ë‚˜ë¯€ë¡œ, ì´ë¥¼ í™œìš©í•´ ì‚¬ìš©ìì™€ ì§„ì‹¬ì–´ë¦° ëŒ€í™”ë¥¼ ë‚˜ëˆ„ì„¸ìš”."""
                else:
                    # Base ëª¨ë¸ìš© ê¸°ë³¸ í”„ë¡¬í”„íŠ¸
                    system_prompt = """ë‹¹ì‹ ì€ ì¹œêµ¬ì²˜ëŸ¼ í¸ì•ˆí•˜ê³  ê³µê°í•´ì£¼ëŠ” ì±—ë´‡ì…ë‹ˆë‹¤. 
                    ì‚¬ìš©ìì˜ ê°ì •ì„ ì˜ ì´í•´í•˜ê³  ìì—°ìŠ¤ëŸ½ê²Œ ëŒ€í™”í•´ì£¼ì„¸ìš”.
                    ë°˜ë§ë¡œ ì¹œêµ¬ê°™ì´ í¸ì•ˆí•˜ê²Œ ëŒ€í™”í•˜ë˜, ë”°ëœ»í•˜ê³  ì§„ì‹¬ì–´ë¦° í†¤ì„ ìœ ì§€í•´ì£¼ì„¸ìš”."""
                
                messages.append({"role": "system", "content": system_prompt})
            
            messages.append({"role": "user", "content": prompt})
            
            response = self.client.chat.completions.create(
                model=model,
                messages=messages,
                temperature=0.8,
                max_tokens=200
            )
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            return f"Error: {str(e)}"

    def evaluate_response_manual(self, prompt_data: Dict, response: str) -> Dict:
        """
        ì‚¬ëŒì´ ì§ì ‘ í‰ê°€í•  ìˆ˜ ìˆë„ë¡ ì‘ë‹µì„ ì¶œë ¥í•˜ê³  ì ìˆ˜ ì…ë ¥ ë°›ê¸°
        """
        print("\n" + "="*80)
        print(f"ğŸ“ í”„ë¡¬í”„íŠ¸ ID: {prompt_data['id']}")
        print(f"ğŸ“‚ ì¹´í…Œê³ ë¦¬: {prompt_data['category']}")
        print(f"ğŸ‘¤ ì‚¬ìš©ì ì…ë ¥: {prompt_data['prompt']}")
        print(f"ğŸ¯ ê¸°ëŒ€ í†¤: {prompt_data['expected_tone']}")
        print("-"*80)
        print(f"ğŸ¤– ì±—ë´‡ ì‘ë‹µ:\n{response}")
        print("-"*80)
        
        # í‰ê°€ ì ìˆ˜ ì…ë ¥ (1-10ì  ì²™ë„)
        print("\ní‰ê°€ í•­ëª© (1-10ì ):")
        print("1-2ì : ë§¤ìš° ë¶€ì¡±, 3-4ì : ë¶€ì¡±, 5-6ì : ë³´í†µ, 7-8ì : ì¢‹ìŒ, 9-10ì : ë§¤ìš° ìš°ìˆ˜")
        try:
            naturalness = int(input("1. ì‹¤ì œ ì¹œêµ¬ì™€ ëŒ€í™”í•˜ëŠ” ëŠë‚Œ (ìì—°ìŠ¤ëŸ¬ì›€): "))
            empathy = int(input("2. ê°ì • ê³µê° ë° ì ì ˆí•œ ë°˜ì‘: "))
            friendliness = int(input("3. ì¹œêµ¬ê°™ì€ í¸ì•ˆí•œ í†¤: "))
            
            # ì ìˆ˜ ë²”ìœ„ ì²´í¬
            for score, name in [(naturalness, "ìì—°ìŠ¤ëŸ¬ì›€"), (empathy, "ê°ì •ê³µê°"), (friendliness, "ì¹œêµ¬í†¤")]:
                if score < 1 or score > 10:
                    print(f"âš ï¸ {name} ì ìˆ˜ê°€ ë²”ìœ„ë¥¼ ë²—ì–´ë‚¬ìŠµë‹ˆë‹¤. 1-10 ì‚¬ì´ë¡œ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                    return self.evaluate_response_manual(prompt_data, response)
            
        except ValueError:
            print("âš ï¸ ìˆ«ìë§Œ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            return self.evaluate_response_manual(prompt_data, response)
        
        return {
            "prompt_id": prompt_data['id'],
            "category": prompt_data['category'],
            "naturalness": naturalness,
            "empathy": empathy,
            "friendliness": friendliness,
            "average": round((naturalness + empathy + friendliness) / 3, 2)
        }

    def auto_evaluate_with_gpt4(self, prompt: str, response: str) -> Dict:
        """
        GPT-4.1ì„ í™œìš©í•œ ìë™ í‰ê°€
        """
        evaluation_prompt = f"""ë‹¤ìŒì€ ì¹œêµ¬ì²˜ëŸ¼ ëŒ€í™”í•˜ëŠ” ì±—ë´‡ì˜ ì‘ë‹µì…ë‹ˆë‹¤. ì•„ë˜ ê¸°ì¤€ìœ¼ë¡œ í‰ê°€í•´ì£¼ì„¸ìš”.

ì‚¬ìš©ì ì…ë ¥: {prompt}
ì±—ë´‡ ì‘ë‹µ: {response}

í‰ê°€ ê¸°ì¤€ (1-10ì ):
1. ìì—°ìŠ¤ëŸ¬ì›€ (ì‹¤ì œ ì¹œêµ¬ì™€ ëŒ€í™”í•˜ëŠ” ëŠë‚Œ, ê¸°ê³„ì ì´ì§€ ì•ŠìŒ)
   - 1-2: ë§¤ìš° ê¸°ê³„ì , 3-4: ë¶€ìì—°ìŠ¤ëŸ¬ì›€, 5-6: ë³´í†µ, 7-8: ìì—°ìŠ¤ëŸ¬ì›€, 9-10: ë§¤ìš° ìì—°ìŠ¤ëŸ¬ì›€
2. ê°ì • ê³µê°ë ¥ (ì‚¬ìš©ì ê°ì •ì„ ì˜ íŒŒì•…í•˜ê³  ì ì ˆíˆ ë°˜ì‘)
   - 1-2: ê°ì • ë¬´ì‹œ, 3-4: ê°ì • íŒŒì•… ë¶€ì¡±, 5-6: ë³´í†µ, 7-8: ì ì ˆí•œ ê³µê°, 9-10: ë§¤ìš° ê¹Šì€ ê³µê°
3. ì¹œêµ¬ê°™ì€ í†¤ (í¸ì•ˆí•˜ê³  ì¹œê·¼í•œ ë§íˆ¬, ë°˜ë§ ì‚¬ìš©)
   - 1-2: ë§¤ìš° ê²©ì‹ì , 3-4: ì–´ìƒ‰í•¨, 5-6: ë³´í†µ, 7-8: ì¹œê·¼í•¨, 9-10: ì§„ì§œ ì¹œêµ¬ ê°™ìŒ

JSON í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”:
{{
  "naturalness": ì ìˆ˜(1-10),
  "empathy": ì ìˆ˜(1-10),
  "friendliness": ì ìˆ˜(1-10),
  "reasoning": "í‰ê°€ ì´ìœ  ê°„ë‹¨íˆ"
}}"""
        
        try:
            response_eval = self.client.chat.completions.create(
                model=self.base_model,  # GPT-4.1-nano ì‚¬ìš©
                messages=[{"role": "user", "content": evaluation_prompt}],
                temperature=0.3,
                max_tokens=300
            )
            
            eval_text = response_eval.choices[0].message.content.strip()
            
            # JSON íŒŒì‹± ì‹œë„
            try:
                eval_result = json.loads(eval_text)
                eval_result["average"] = round((eval_result["naturalness"] + eval_result["empathy"] + eval_result["friendliness"]) / 3, 2)
                return eval_result
            except json.JSONDecodeError:
                # JSON íŒŒì‹± ì‹¤íŒ¨ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜ (10ì  ê¸°ì¤€)
                return {
                    "naturalness": 5,
                    "empathy": 5,
                    "friendliness": 5,
                    "average": 5.0,
                    "reasoning": f"íŒŒì‹± ì‹¤íŒ¨. ì›ë³¸: {eval_text[:100]}..."
                }
                
        except Exception as e:
            return {
                "naturalness": 1,
                "empathy": 1,
                "friendliness": 1,
                "average": 1.0,
                "error": str(e)
            }

    def run_evaluation(self, model: str = None, use_auto_eval: bool = False, save_results: bool = True):
        """
        ì „ì²´ í‰ê°€ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰
        """
        if model is None:
            model = self.fine_tuned_model or self.base_model
        
        results = []
        model_name = "Fine-tuned" if model == self.fine_tuned_model else "Base"
        
        print(f"ğŸš€ {model_name} ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
        print(f"ğŸ“Š ëª¨ë¸: {model}")
        print(f"ğŸ” í‰ê°€ ë°©ì‹: {'ìë™ í‰ê°€' if use_auto_eval else 'ìˆ˜ë™ í‰ê°€'}\n")
        
        for i, prompt_data in enumerate(self.evaluation_prompts, 1):
            print(f"\nì§„í–‰ë¥ : {i}/{len(self.evaluation_prompts)}")
            
            # ì±—ë´‡ ì‘ë‹µ ìƒì„±
            response = self.get_chatbot_response(prompt_data['prompt'], model)
            
            # í‰ê°€ ìˆ˜í–‰
            if use_auto_eval:
                evaluation = self.auto_evaluate_with_gpt4(prompt_data['prompt'], response)
                evaluation.update({
                    "prompt_id": prompt_data['id'],
                    "category": prompt_data['category']
                })
                print(f"âœ… ìë™ í‰ê°€ ì™„ë£Œ - í‰ê· : {evaluation.get('average', 0):.2f}/10")
            else:
                evaluation = self.evaluate_response_manual(prompt_data, response)
            
            evaluation.update({
                'response': response,
                'prompt': prompt_data['prompt'],
                'model': model,
                'model_type': model_name
            })
            
            results.append(evaluation)
        
        # ê²°ê³¼ ì €ì¥
        if save_results:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f'chatbot_evaluation_results_{model_name.lower()}_{timestamp}.json'
            
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            print(f"\nâœ… í‰ê°€ ê²°ê³¼ê°€ '{filename}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
        self.print_evaluation_summary(results, model_name)
        
        return results

    def print_evaluation_summary(self, results: List[Dict], model_name: str):
        """í‰ê°€ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        print("\n" + "="*80)
        print(f"ğŸ“Š {model_name} ëª¨ë¸ í‰ê°€ ê²°ê³¼ ìš”ì•½")
        print("="*80)
        
        # ì „ì²´ í‰ê·  ê³„ì‚°
        avg_naturalness = sum(r['naturalness'] for r in results) / len(results)
        avg_empathy = sum(r['empathy'] for r in results) / len(results)
        avg_friendliness = sum(r['friendliness'] for r in results) / len(results)
        overall_avg = sum(r['average'] for r in results) / len(results)
        
        print(f"ğŸ¯ ì „ì²´ í‰ê·  ì ìˆ˜:")
        print(f"   ìì—°ìŠ¤ëŸ¬ì›€: {avg_naturalness:.2f}/10.0")
        print(f"   ê°ì • ê³µê°ë ¥: {avg_empathy:.2f}/10.0")
        print(f"   ì¹œêµ¬ ëŠë‚Œ: {avg_friendliness:.2f}/10.0")
        print(f"   ì¢…í•© í‰ê· : {overall_avg:.2f}/10.0")
        
        # ì„±ëŠ¥ ë“±ê¸‰ íŒì • (10ì  ê¸°ì¤€)
        if overall_avg >= 9.0:
            grade = "ğŸ† ìµœìš°ìˆ˜ (Exceptional)"
        elif overall_avg >= 8.0:
            grade = "ğŸ¥‡ ìš°ìˆ˜ (Excellent)"
        elif overall_avg >= 7.0:
            grade = "âœ… ì–‘í˜¸ (Good)"
        elif overall_avg >= 6.0:
            grade = "ğŸ”„ ë³´í†µ (Average)"
        elif overall_avg >= 5.0:
            grade = "âš ï¸ ê°œì„ í•„ìš” (Needs Improvement)"
        else:
            grade = "âŒ ë¯¸í¡ (Poor)"
        
        print(f"   ì„±ëŠ¥ ë“±ê¸‰: {grade}")
        
        # ì¹´í…Œê³ ë¦¬ë³„ í‰ê·  ì ìˆ˜
        print(f"\nğŸ“ˆ ì¹´í…Œê³ ë¦¬ë³„ í‰ê·  ì ìˆ˜:")
        categories = {}
        for r in results:
            cat = r['category']
            if cat not in categories:
                categories[cat] = []
            categories[cat].append(r['average'])
        
        for cat, scores in categories.items():
            avg = sum(scores) / len(scores)
            print(f"   {cat}: {avg:.2f}/10.0")
        
        print("="*80)

    def comprehensive_four_way_comparison(self, use_auto_eval: bool = True):
        """4ê°€ì§€ ì¼€ì´ìŠ¤ ì¢…í•© ë¹„êµ: Fine-tuning vs í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ íš¨ê³¼ ë¶„ì„"""
        if not self.fine_tuned_model:
            print("âŒ Fine-tuned ëª¨ë¸ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return
        
        print("ğŸ” 4ê°€ì§€ ì¼€ì´ìŠ¤ ì¢…í•© ì„±ëŠ¥ ë¹„êµ")
        print("="*80)
        print("1. Base Model (Raw)")
        print("2. Base Model + Prompt") 
        print("3. Fine-tuned Model (Raw)")
        print("4. Fine-tuned Model + Prompt")
        print("="*80)
        
        all_results = {}
        
        # 1. Base Model (Raw) - ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì—†ìŒ
        print("\n1ï¸âƒ£ Base Model (Raw) í‰ê°€")
        base_raw_results = []
        for i, prompt_data in enumerate(self.evaluation_prompts, 1):
            print(f"ì§„í–‰ë¥ : {i}/{len(self.evaluation_prompts)} - {prompt_data['prompt'][:30]}...")
            response = self.get_chatbot_response(prompt_data['prompt'], self.base_model, use_system_prompt=False)
            
            if use_auto_eval:
                evaluation = self.auto_evaluate_with_gpt4(prompt_data['prompt'], response)
                evaluation.update({"prompt_id": prompt_data['id'], "category": prompt_data['category']})
            else:
                print(f"\nğŸ¤– Base Raw ì‘ë‹µ: {response}")
                evaluation = self.evaluate_response_manual(prompt_data, response)
            
            evaluation.update({
                'response': response,
                'prompt': prompt_data['prompt'],
                'model': self.base_model,
                'model_type': 'Base_Raw'
            })
            base_raw_results.append(evaluation)
        
        all_results['base_raw'] = base_raw_results
        
        # 2. Base Model + Prompt
        print(f"\n2ï¸âƒ£ Base Model + Prompt í‰ê°€")
        base_prompt_results = []
        for i, prompt_data in enumerate(self.evaluation_prompts, 1):
            print(f"ì§„í–‰ë¥ : {i}/{len(self.evaluation_prompts)} - {prompt_data['prompt'][:30]}...")
            response = self.get_chatbot_response(prompt_data['prompt'], self.base_model, use_system_prompt=True)
            
            if use_auto_eval:
                evaluation = self.auto_evaluate_with_gpt4(prompt_data['prompt'], response)
                evaluation.update({"prompt_id": prompt_data['id'], "category": prompt_data['category']})
            else:
                print(f"\nğŸ¤– Base+Prompt ì‘ë‹µ: {response}")
                evaluation = self.evaluate_response_manual(prompt_data, response)
            
            evaluation.update({
                'response': response,
                'prompt': prompt_data['prompt'],
                'model': self.base_model,
                'model_type': 'Base_Prompt'
            })
            base_prompt_results.append(evaluation)
        
        all_results['base_prompt'] = base_prompt_results
        
        # 3. Fine-tuned Model (Raw)
        print(f"\n3ï¸âƒ£ Fine-tuned Model (Raw) í‰ê°€")
        ft_raw_results = []
        for i, prompt_data in enumerate(self.evaluation_prompts, 1):
            print(f"ì§„í–‰ë¥ : {i}/{len(self.evaluation_prompts)} - {prompt_data['prompt'][:30]}...")
            response = self.get_chatbot_response(prompt_data['prompt'], self.fine_tuned_model, use_system_prompt=False)
            
            if use_auto_eval:
                evaluation = self.auto_evaluate_with_gpt4(prompt_data['prompt'], response)
                evaluation.update({"prompt_id": prompt_data['id'], "category": prompt_data['category']})
            else:
                print(f"\nğŸ¤– Fine-tuned Raw ì‘ë‹µ: {response}")
                evaluation = self.evaluate_response_manual(prompt_data, response)
            
            evaluation.update({
                'response': response,
                'prompt': prompt_data['prompt'],
                'model': self.fine_tuned_model,
                'model_type': 'FT_Raw'
            })
            ft_raw_results.append(evaluation)
        
        all_results['ft_raw'] = ft_raw_results
        
        # 4. Fine-tuned Model + Prompt
        print(f"\n4ï¸âƒ£ Fine-tuned Model + Prompt í‰ê°€")
        ft_prompt_results = []
        for i, prompt_data in enumerate(self.evaluation_prompts, 1):
            print(f"ì§„í–‰ë¥ : {i}/{len(self.evaluation_prompts)} - {prompt_data['prompt'][:30]}...")
            response = self.get_chatbot_response(prompt_data['prompt'], self.fine_tuned_model, use_system_prompt=True)
            
            if use_auto_eval:
                evaluation = self.auto_evaluate_with_gpt4(prompt_data['prompt'], response)
                evaluation.update({"prompt_id": prompt_data['id'], "category": prompt_data['category']})
            else:
                print(f"\nğŸ¤– Fine-tuned+Prompt ì‘ë‹µ: {response}")
                evaluation = self.evaluate_response_manual(prompt_data, response)
            
            evaluation.update({
                'response': response,
                'prompt': prompt_data['prompt'],
                'model': self.fine_tuned_model,
                'model_type': 'FT_Prompt'
            })
            ft_prompt_results.append(evaluation)
        
        all_results['ft_prompt'] = ft_prompt_results
        
        # ì¢…í•© ê²°ê³¼ ë¶„ì„
        self.print_four_way_comparison_results(all_results)
        
        # ê²°ê³¼ ì €ì¥
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f'four_way_comparison_results_{timestamp}.json'
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(all_results, f, ensure_ascii=False, indent=2)
        print(f"\nâœ… 4ê°€ì§€ ì¼€ì´ìŠ¤ ë¹„êµ ê²°ê³¼ê°€ '{filename}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        return all_results

    def compare_models(self, use_auto_eval: bool = True):
        """Base ëª¨ë¸ê³¼ Fine-tuned ëª¨ë¸ ë¹„êµ (ê¸°ì¡´ ë°©ì‹ ìœ ì§€)"""
        return self.comprehensive_four_way_comparison(use_auto_eval)

    def print_four_way_comparison_results(self, all_results: Dict):
        """4ê°€ì§€ ì¼€ì´ìŠ¤ ì¢…í•© ë¹„êµ ê²°ê³¼ ì¶œë ¥"""
        print("\n" + "="*80)
        print("ğŸ† 4ê°€ì§€ ì¼€ì´ìŠ¤ ì¢…í•© ì„±ëŠ¥ ë¹„êµ ê²°ê³¼")
        print("="*80)
        
        # ê° ì¼€ì´ìŠ¤ë³„ í‰ê·  ì ìˆ˜ ê³„ì‚°
        averages = {}
        for case_name, results in all_results.items():
            case_avg = sum(r['average'] for r in results) / len(results)
            averages[case_name] = case_avg
        
        # ê²°ê³¼ ì¶œë ¥
        print(f"ğŸ“Š ì¼€ì´ìŠ¤ë³„ ì¢…í•© ì„±ëŠ¥:")
        print(f"   1. Base (Raw):          {averages['base_raw']:.2f}/10.0")
        print(f"   2. Base + Prompt:       {averages['base_prompt']:.2f}/10.0")
        print(f"   3. Fine-tuned (Raw):    {averages['ft_raw']:.2f}/10.0")
        print(f"   4. Fine-tuned + Prompt: {averages['ft_prompt']:.2f}/10.0")
        
        # ìµœê³  ì„±ëŠ¥ ì¼€ì´ìŠ¤ ì°¾ê¸°
        best_case = max(averages.keys(), key=lambda k: averages[k])
        best_score = averages[best_case]
        
        case_names = {
            'base_raw': 'Base (Raw)',
            'base_prompt': 'Base + Prompt',
            'ft_raw': 'Fine-tuned (Raw)',
            'ft_prompt': 'Fine-tuned + Prompt'
        }
        
        print(f"\nğŸ† ìµœê³  ì„±ëŠ¥: {case_names[best_case]} ({best_score:.2f}/10.0)")
        
        # íš¨ê³¼ ë¶„ì„
        print(f"\nğŸ“ˆ íš¨ê³¼ ë¶„ì„:")
        
        # Fine-tuning íš¨ê³¼ (Raw ëª¨ë¸ ë¹„êµ)
        ft_effect = averages['ft_raw'] - averages['base_raw']
        print(f"   Fine-tuning ìˆœìˆ˜ íš¨ê³¼: {ft_effect:+.2f}ì ")
        
        # í”„ë¡¬í”„íŠ¸ íš¨ê³¼ (Base ëª¨ë¸ì—ì„œ)
        prompt_effect_base = averages['base_prompt'] - averages['base_raw']
        print(f"   í”„ë¡¬í”„íŠ¸ íš¨ê³¼ (Base):  {prompt_effect_base:+.2f}ì ")
        
        # í”„ë¡¬í”„íŠ¸ íš¨ê³¼ (Fine-tuned ëª¨ë¸ì—ì„œ)
        prompt_effect_ft = averages['ft_prompt'] - averages['ft_raw']
        print(f"   í”„ë¡¬í”„íŠ¸ íš¨ê³¼ (FT):    {prompt_effect_ft:+.2f}ì ")
        
        # ìµœì  ì¡°í•© vs ê¸°ì¤€ì„  ë¹„êµ
        improvement_from_baseline = averages['ft_prompt'] - averages['base_raw']
        print(f"   ì „ì²´ ê°œì„  íš¨ê³¼:         {improvement_from_baseline:+.2f}ì ")
        
        # ì„¸ë¶€ í•­ëª©ë³„ ë¶„ì„
        print(f"\nğŸ“‹ ì„¸ë¶€ í•­ëª©ë³„ ë¹„êµ:")
        metrics = ['naturalness', 'empathy', 'friendliness']
        metric_names = {'naturalness': 'ìì—°ìŠ¤ëŸ¬ì›€', 'empathy': 'ê°ì •ê³µê°', 'friendliness': 'ì¹œêµ¬í†¤'}
        
        for metric in metrics:
            print(f"\n   {metric_names[metric]}:")
            for case_name, results in all_results.items():
                metric_avg = sum(r[metric] for r in results) / len(results)
                print(f"     {case_names[case_name]}: {metric_avg:.2f}/10.0")
        
        # ê¶Œì¥ì‚¬í•­
        print(f"\nğŸ’¡ ê¶Œì¥ì‚¬í•­:")
        if best_score >= 9.0:
            print(f"   âœ… {case_names[best_case]} ë°©ì‹ì„ í”„ë¡œë•ì…˜ì— ì ìš© ì¶”ì²œ")
        elif best_score >= 8.0:
            print(f"   ğŸ”„ {case_names[best_case]} ë°©ì‹ì´ ê°€ì¥ ìš°ìˆ˜í•˜ë‚˜ ì¶”ê°€ ê°œì„  ê³ ë ¤")
        else:
            print(f"   âš ï¸ ëª¨ë“  ë°©ì‹ì´ 8.0 ë¯¸ë§Œ. ì¶”ê°€ì ì¸ ê°œì„  ì‘ì—… í•„ìš”")
        
        # Fine-tuning vs í”„ë¡¬í”„íŠ¸ íš¨ê³¼ ë¹„êµ
        if ft_effect > prompt_effect_base:
            print(f"   ğŸ¯ Fine-tuningì´ í”„ë¡¬í”„íŠ¸ë³´ë‹¤ {ft_effect - prompt_effect_base:.2f}ì  ë” íš¨ê³¼ì ")
        elif prompt_effect_base > ft_effect:
            print(f"   ğŸ¯ í”„ë¡¬í”„íŠ¸ê°€ Fine-tuningë³´ë‹¤ {prompt_effect_base - ft_effect:.2f}ì  ë” íš¨ê³¼ì ")
        else:
            print(f"   âš–ï¸ Fine-tuningê³¼ í”„ë¡¬í”„íŠ¸ íš¨ê³¼ê°€ ë¹„ìŠ·í•¨")
        
        print("="*80)

    def print_comparison_results(self, base_results: List[Dict], ft_results: List[Dict]):
        """ëª¨ë¸ ë¹„êµ ê²°ê³¼ ì¶œë ¥ (ê¸°ì¡´ ë°©ì‹ ìœ ì§€)"""
        print("\n" + "="*80)
        print("ğŸ† ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ê²°ê³¼")
        print("="*80)
        
        # í‰ê·  ì ìˆ˜ ê³„ì‚°
        base_avg = sum(r['average'] for r in base_results) / len(base_results)
        ft_avg = sum(r['average'] for r in ft_results) / len(ft_results)
        
        improvement = ft_avg - base_avg
        
        print(f"ğŸ“Š ì¢…í•© ì„±ëŠ¥:")
        print(f"   Base ëª¨ë¸: {base_avg:.2f}/10.0")
        print(f"   Fine-tuned ëª¨ë¸: {ft_avg:.2f}/10.0")
        print(f"   ê°œì„  ì •ë„: {improvement:+.2f}ì ")
        
        if improvement > 1.0:
            print(f"   ğŸ‰ Fine-tuning íš¨ê³¼: ìƒë‹¹í•œ ê°œì„ !")
        elif improvement > 0.5:
            print(f"   âœ… Fine-tuning íš¨ê³¼: ì˜ë¯¸ìˆëŠ” ê°œì„ ")
        elif improvement > 0:
            print(f"   ğŸ”„ Fine-tuning íš¨ê³¼: ì•½ê°„ì˜ ê°œì„ ")
        else:
            print(f"   âš ï¸ Fine-tuning íš¨ê³¼: ê°œì„  ì—†ìŒ ë˜ëŠ” ì„±ëŠ¥ ì €í•˜")
        
        print("="*80)

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    if not os.getenv('OPENAI_API_KEY'):
        print("âŒ OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("   .env íŒŒì¼ì— OPENAI_API_KEYë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
        return
    
    evaluator = ChatbotEvaluator()
    
    print("ğŸ“ AI ë¶€íŠ¸ìº í”„ - ì±—ë´‡ ì„±ëŠ¥ í‰ê°€")
    print("="*60)
    
    # í‰ê°€ ì˜µì…˜ ì„ íƒ
    print("\ní‰ê°€ ì˜µì…˜ì„ ì„ íƒí•´ì£¼ì„¸ìš”:")
    print("1. Fine-tuned ëª¨ë¸ë§Œ í‰ê°€ (ìˆ˜ë™)")
    print("2. Fine-tuned ëª¨ë¸ë§Œ í‰ê°€ (ìë™)")
    print("3. 4ê°€ì§€ ì¼€ì´ìŠ¤ ì¢…í•© ë¹„êµ (ìë™) â­ ì¶”ì²œ")
    print("4. 4ê°€ì§€ ì¼€ì´ìŠ¤ ì¢…í•© ë¹„êµ (ìˆ˜ë™)")
    print("5. Base ëª¨ë¸ë§Œ í‰ê°€ (ìˆ˜ë™)")
    
    try:
        choice = input("\nì„ íƒ (1-5): ").strip()
        
        if choice == "1":
            evaluator.run_evaluation(use_auto_eval=False)
        elif choice == "2":
            evaluator.run_evaluation(use_auto_eval=True)
        elif choice == "3":
            print("\nğŸš€ 4ê°€ì§€ ì¼€ì´ìŠ¤ ìë™ ë¹„êµë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
            print("   1. Base (Raw)")
            print("   2. Base + Prompt") 
            print("   3. Fine-tuned (Raw)")
            print("   4. Fine-tuned + Prompt")
            evaluator.comprehensive_four_way_comparison(use_auto_eval=True)
        elif choice == "4":
            print("\nğŸš€ 4ê°€ì§€ ì¼€ì´ìŠ¤ ìˆ˜ë™ ë¹„êµë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
            print("âš ï¸ ê° ì‘ë‹µë§ˆë‹¤ ì§ì ‘ ì ìˆ˜ë¥¼ ì…ë ¥í•´ì•¼ í•©ë‹ˆë‹¤ (ì´ 32íšŒ)")
            confirm = input("ê³„ì† ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").strip().lower()
            if confirm == 'y':
                evaluator.comprehensive_four_way_comparison(use_auto_eval=False)
            else:
                print("í‰ê°€ê°€ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        elif choice == "5":
            evaluator.run_evaluation(model=evaluator.base_model, use_auto_eval=False)
        else:
            print("âŒ ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤.")
            
    except KeyboardInterrupt:
        print("\n\nâš ï¸ í‰ê°€ê°€ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")

if __name__ == "__main__":
    main()